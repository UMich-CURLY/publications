%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@STRING{C-AAAI = {Proceedings of the {AAAI} National Conference on Artificial Intelligence}}
@STRING{C-ACC = {Proceedings of the American Control Conference}}
@STRING{C-AGU = {EOS: Transactions of the American Geophysical Union Fall Meeting Supplement}}
@STRING{C-AUV = {Proceedings of the {IEEE/OES} Autonomous Underwater Vehicles Conference}}
@STRING{C-BMVC = {Proceedings of the British Machine Vision Conference}}
@STRING{C-CDC = {Proceedings of the {IEEE} Conference on Decision and Control}}
@STRING{C-CORL = {Proceedings of the Conference on Robot Learning}}
@STRING{C-CVPR = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition}}
@STRING{C-DICTA = {Proceedings of Digital Image Computing: Techniques and Applications}}
@STRING{C-ECCV = {Proceedings of the European Conference on Computer Vision}}
@STRING{C-ECMR = {Proceedings of the European Conference on Mobile Robotics}}
@STRING{C-FSR = {Proceedings of the International Conference on Field and Service Robotics}}
@STRING{C-ICCV = {Proceedings of the {IEEE} International Conference on Computer Vision}}
@STRING{C-ICIP = {Proceedings of the International Conference on Image Processing}}
@STRING{C-ICPR = {Proceedings of the International Conference Pattern Recognition}}
@STRING{C-ICRA = {Proceedings of the {IEEE} International Conference on Robotics and Automation}}
@STRING{C-IJCAI = {Proceedings of the International Joint Conference on Artifical Intelligence}}
@STRING{C-IROS = {Proceedings of the {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems}}
@STRING{C-ISCV = {Proceedings of the International Symposium on Computer Vision}}
@STRING{C-ISER = {Proceedings of the International Symposium on Experimental Robotics}}
@STRING{C-ISRR = {Proceedings of the International Symposium on Robotics Research}}
@STRING{C-ISUT = {Proceedings of the International Symposium on Underwater Technology}}
@STRING{C-IVS  = {Proceedings of the {IEEE} Intelligent Vehicle Symposium}}
@STRING{C-NIPS = {Proceedings of the Advances in Neural Information Processing Systems Conference}}
@STRING{C-OCEANS = {Proceedings of the {IEEE}/{MTS} {OCEANS} Conference and Exhibition}}
@STRING{C-OCEANS-Europe = {Proceedings of the {IEEE} {OCEANS}-Europe Conference and Exhibition}}
@STRING{C-RSS = {Proceedings of the Robotics: Science and Systems Conference}}
@STRING{C-UAI = {Proceedings of Uncertainty in AI}}
@STRING{C-UUST = {Proceedings of the International Symposium on Unmanned Untethered Submersible Technology}}
@STRING{C-WAFR = {Proceedings of the International Workshop on the Algorithmic Foundations of Robotics}}
@STRING{C-WUNET = {Proceedings of the ACM International Conference on Underwater Networks and Systems (WUWNet)}}


@STRING{I-CALTECH = {California Institue of Technology}}
@STRING{I-CMU = {Carnegie Mellon University}}
@STRING{I-JHU = {Johns Hopkins University}}
@STRING{I-KTH = {Katholieke Universiteit Leuven}}
@STRING{I-MIT = {Massachusetts Institute of Technology}}
@STRING{I-MIT/WHOI = {Massachusetts Institute of Technology / Woods Hole Oceanographic Institution Joint Program}}
@STRING{I-SCRIPPS = {Scripps Institution of Oceanography}}
@STRING{I-UMICH = {University of Michigan}}
@STRING{I-WHOI = {Woods Hole Oceanographic Institution}}
@STRING{IEEE_J_AC         = "{IEEE} Transactions on Automatic Control"}



% components, packaging and manufacturing
@STRING{IEEE_J_ADVP       = "{IEEE} Transactions on Advanced Packaging"}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IEEE Journals 



% aerospace and military
@STRING{IEEE_J_AES        = "{IEEE} Transactions on Aerospace and Electronic Systems"}
@STRING{IEEE_J_AIRE       = "{IEEE} Transactions on Airborne Electronics"}
@STRING{IEEE_J_ANE        = "{IEEE} Transactions on Aerospace and Navigational Electronics"}
@STRING{IEEE_J_ANNE       = "{IEEE} Transactions on Aeronautical and Navigational Electronics"}
@STRING{IEEE_J_AP         = "{IEEE} Transactions on Antennas and Propagation"}



% industrial, commercial and consumer
@STRING{IEEE_J_APPIND     = "{IEEE} Transactions on Applications and Industry"}
@STRING{IEEE_J_AS         = "{IEEE} Transactions on Aerospace"}
@STRING{IEEE_J_ASC        = "{IEEE} Transactions on Applied Superconductivity"}



% cybernetics, ergonomics, robots, man-machine, and automation
@STRING{IEEE_J_ASE        = "{IEEE} Transactions on Automation Science and Engineering"}
@STRING{IEEE_J_ASSP       = "{IEEE} Transactions on Acoustics, Speech, and Signal Processing"}
@STRING{IEEE_J_AU         = "{IEEE} Transactions on Audio"}
@STRING{IEEE_J_AUEA       = "{IEEE} Transactions on Audio and Electroacoustics"}



% electromagnetics, antennas, EMI, magnetics and microwave
@STRING{IEEE_J_AWPL       = "{IEEE} Antennas and Wireless Propagation Letters"}
% Note: The B-ME journal later dropped the hyphen and became the BME.
@STRING{IEEE_J_B-ME       = "{IEEE} Transactions on Bio-Medical Engineering"}
@STRING{IEEE_J_BC         = "{IEEE} Transactions on Broadcasting"}



% medical and biological
@STRING{IEEE_J_BCAS       = "{IEEE} Transactions on Biomedical Circuits and Systems"}
@STRING{IEEE_J_BCTV       = "{IEEE} Transactions on Broadcast and Television Receivers"}
@STRING{IEEE_J_BME        = "{IEEE} Transactions on Biomedical Engineering"}
@STRING{IEEE_J_BMELC      = "{IEEE} Transactions on Bio-Medical Electronics"}



% computers, computation, networking and software
@STRING{IEEE_J_C          = "{IEEE} Transactions on Computers"}
@STRING{IEEE_J_CAD        = "{IEEE} Transactions on Computer-Aided Design of Integrated Circuits and Systems"}
@STRING{IEEE_J_CAL        = "{IEEE} Computer Architecture Letters"}
@STRING{IEEE_J_CAPT       = "{IEEE} Transactions on Components and Packaging Technology"}
@STRING{IEEE_J_CAPTS      = "{IEEE} Transactions on Components and Packaging Technologies"}
@STRING{IEEE_J_CAS        = "{IEEE} Transactions on Circuits and Systems"}
@STRING{IEEE_J_CASI       = "{IEEE} Transactions on Circuits and Systems---Part {I}: Fundamental Theory and Applications"}
% in 2004 CASI and CASII renamed part title to CASI_RP and CASII_EB, respectively.
@STRING{IEEE_J_CASI_RP    = "{IEEE} Transactions on Circuits and Systems---Part {I}: Regular Papers"}
@STRING{IEEE_J_CASII      = "{IEEE} Transactions on Circuits and Systems---Part {II}: Analog and Digital Signal Processing"}
@STRING{IEEE_J_CASII_EB   = "{IEEE} Transactions on Circuits and Systems---Part {II}: Express Briefs"}
@STRING{IEEE_J_CASVT      = "{IEEE} Transactions on Circuits and Systems for Video Technology"}
@STRING{IEEE_J_CBB        = "{IEEE/ACM} Transactions on Computational Biology and Bioinformatics"}
@STRING{IEEE_J_CE         = "{IEEE} Transactions on Consumer Electronics"}
@STRING{IEEE_J_CHMT       = "{IEEE} Transactions on Components, Hybrids and Manufacturing Technology"}



% education, engineering, history, IEEE, professional
@STRING{IEEE_J_CJECE      = "Canadian Journal of Electrical and Computer Engineering"}
@STRING{IEEE_J_COM        = "{IEEE} Transactions on Communications"}



% communications
@STRING{IEEE_J_COML       = "{IEEE} Communications Letters"}
@STRING{IEEE_J_COMT       = "{IEEE} Transactions on Communication Technology"}
@STRING{IEEE_J_CPART      = "{IEEE} Transactions on Component Parts"}
@STRING{IEEE_J_CPMTA      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {A}"}
@STRING{IEEE_J_CPMTB      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {B}: Advanced Packaging"}
@STRING{IEEE_J_CPMTC      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {C}: Manufacturing"}
@STRING{IEEE_J_CST        = "{IEEE} Transactions on Control Systems Technology"}
@STRING{IEEE_J_CT         = "{IEEE} Transactions on Circuit Theory"}
@STRING{IEEE_J_DEI        = "{IEEE} Transactions on Dielectrics and Electrical Insulation"}



% reliability
@STRING{IEEE_J_DMR        = "{IEEE} Transactions on Device and Materials Reliability"}
@STRING{IEEE_J_DSC        = "{IEEE} Transactions on Dependable and Secure Computing"}



% energy and power
@STRING{IEEE_J_EC         = "{IEEE} Transactions on Energy Conversion"}
@STRING{IEEE_J_ECOMP      = "{IEEE} Transactions on Electronic Computers"}
@STRING{IEEE_J_ED         = "{IEEE} Transactions on Electron Devices"}



% physics, electrons, nanotechnology, nuclear and quantum electronics
@STRING{IEEE_J_EDL        = "{IEEE} Electron Device Letters"}
@STRING{IEEE_J_EDU        = "{IEEE} Transactions on Education"}
@STRING{IEEE_J_EI         = "{IEEE} Transactions on Electrical Insulation"}
@STRING{IEEE_J_EM         = "{IEEE} Transactions on Engineering Management"}
@STRING{IEEE_J_EMC        = "{IEEE} Transactions on Electromagnetic Compatibility"}
@STRING{IEEE_J_EPM        = "{IEEE} Transactions on Electronics Packaging Manufacturing"}



% semiconductors, superconductors, electrochemical and solid state
@STRING{IEEE_J_ESSL       = "{IEEE/ECS} Electrochemical and Solid-State Letters"}
@STRING{IEEE_J_EVC        = "{IEEE} Transactions on Evolutionary Computation"}
@STRING{IEEE_J_EWS        = "{IEEE} Transactions on Engineering Writing and Speech"}
@STRING{IEEE_J_FUZZ       = "{IEEE} Transactions on Fuzzy Systems"}



% earth, wind, fire and water
@STRING{IEEE_J_GE         = "{IEEE} Transactions on Geoscience Electronics"}
@STRING{IEEE_J_GRS        = "{IEEE} Transactions on Geoscience and Remote Sensing"}
@STRING{IEEE_J_GRSL       = "{IEEE} Geoscience and Remote Sensing Letters"}
@STRING{IEEE_J_H          = "{IEEE} Transactions on Haptics"}
@STRING{IEEE_J_HFE        = "{IEEE} Transactions on Human Factors in Electronics"}
@STRING{IEEE_J_IA         = "{IEEE} Transactions on Industry Applications"}
@STRING{IEEE_J_IE         = "{IEEE} Transactions on Industrial Electronics"}
@STRING{IEEE_J_IECI       = "{IEEE} Transactions on Industrial Electronics and Control Instrumentation"}
@STRING{IEEE_J_IFS        = "{IEEE} Transactions on Information Forensics and Security"}
@STRING{IEEE_J_IGA        = "{IEEE} Transactions on Industry and General Applications"}
@STRING{IEEE_J_IINF       = "{IEEE} Transactions on Industrial Informatics"}



% instrumentation and measurement
@STRING{IEEE_J_IM         = "{IEEE} Transactions on Instrumentation and Measurement"}
@STRING{IEEE_J_IP         = "{IEEE} Transactions on Image Processing"}



% coding, data, information, knowledge
@STRING{IEEE_J_IT         = "{IEEE} Transactions on Information Theory"}
@STRING{IEEE_J_ITBM       = "{IEEE} Transactions on Information Technology in Biomedicine"}



% autos, transportation and vehicles (non-aerospace)
@STRING{IEEE_J_ITS        = "{IEEE} Transactions on Intelligent Transportation Systems"}



% computer graphics, imaging, and multimedia
@STRING{IEEE_J_JDT        = "{IEEE/OSA} Journal of Display Technology"}



% insulation and materials
@STRING{IEEE_J_JEM        = "{IEEE/TMS} Journal of Electronic Materials"}
@STRING{IEEE_J_JLT        = "{IEEE/OSA} Journal of Lightwave Technology"}
@STRING{IEEE_J_JQE        = "{IEEE} Journal of Quantum Electronics"}
@STRING{IEEE_J_JRA        = "{IEEE} Journal of Robotics and Automation"}
@STRING{IEEE_J_JSAC       = "{IEEE} Journal on Selected Areas in Communications"}
@STRING{IEEE_J_JSSC       = "{IEEE} Journal of Solid-State Circuits"}
@STRING{IEEE_J_JSTQE      = "{IEEE} Journal of Selected Topics in Quantum Electronics"}
@STRING{IEEE_J_KDE        = "{IEEE} Transactions on Knowledge and Data Engineering"}
@STRING{IEEE_J_LT         = "{IEEE} Transactions on Learning Technologies"}
@STRING{IEEE_J_MAG        = "{IEEE} Transactions on Magnetics"}
@STRING{IEEE_J_MC         = "{IEEE} Transactions on Mobile Computing"}
@STRING{IEEE_J_ME         = "{IEEE} Transactions on Medical Electronics"}



% mechanical
@STRING{IEEE_J_MECH       = "{IEEE/ASME} Transactions on Mechatronics"}
@STRING{IEEE_J_MEMS       = "{IEEE/ASME} Journal of Microelectromechanical Systems"}
@STRING{IEEE_J_MFT        = "{IEEE} Transactions on Manufacturing Technology"}
@STRING{IEEE_J_MGWL       = "{IEEE} Microwave and Guided Wave Letters"}
@STRING{IEEE_J_MI         = "{IEEE} Transactions on Medical Imaging"}
@STRING{IEEE_J_MIL        = "{IEEE} Transactions on Military Electronics"}
@STRING{IEEE_J_MM         = "{IEEE} Transactions on Multimedia"}
@STRING{IEEE_J_MMS        = "{IEEE} Transactions on Man-Machine Systems"}
@STRING{IEEE_J_MTT        = "{IEEE} Transactions on Microwave Theory and Techniques"}
@STRING{IEEE_J_MWCL       = "{IEEE} Microwave and Wireless Components Letters"}
@STRING{IEEE_J_NANO       = "{IEEE} Transactions on Nanotechnology"}
@STRING{IEEE_J_NB         = "{IEEE} Transactions on NanoBioscience"}
@STRING{IEEE_J_NET        = "{IEEE/ACM} Transactions on Networking"}
@STRING{IEEE_J_NN         = "{IEEE} Transactions on Neural Networks"}
@STRING{IEEE_J_NS         = "{IEEE} Transactions on Nuclear Science"}
@STRING{IEEE_J_NSM        = "{IEEE} Transactions on Network and Service Management"}
@STRING{IEEE_J_NSRE       = "{IEEE} Transactions on Neural Systems and Rehabilitation Engineering"}
@STRING{IEEE_J_OE         = "{IEEE} Journal of Oceanic Engineering"}
@STRING{IEEE_J_PAMI       = "{IEEE} Transactions on Pattern Analysis and Machine Intelligence"}
@STRING{IEEE_J_PC         = "{IEEE} Transactions on Professional Communication"}
@STRING{IEEE_J_PDS        = "{IEEE} Transactions on Parallel and Distributed Systems"}
@STRING{IEEE_J_PEL        = "{IEEE} Power Electronics Letters"}
@STRING{IEEE_J_PHP        = "{IEEE} Transactions on Parts, Hybrids and Packaging"}
@STRING{IEEE_J_PMP        = "{IEEE} Transactions on Parts, Materials and Packaging"}
@STRING{IEEE_J_PROC       = "Proceedings of the {IEEE}"}
@STRING{IEEE_J_PS         = "{IEEE} Transactions on Plasma Science"}
@STRING{IEEE_J_PSE        = "{IEEE} Journal of Product Safety Engineering"}



% optics, lightwave and photonics
@STRING{IEEE_J_PTL        = "{IEEE} Photonics Technology Letters"}
@STRING{IEEE_J_PWRAS      = "{IEEE} Transactions on Power Apparatus and Systems"}
@STRING{IEEE_J_PWRD       = "{IEEE} Transactions on Power Delivery"}
@STRING{IEEE_J_PWRE       = "{IEEE} Transactions on Power Electronics"}
@STRING{IEEE_J_PWRS       = "{IEEE} Transactions on Power Systems"}
@STRING{IEEE_J_R          = "{IEEE} Transactions on Reliability"}
% in 1989 JRA became RA
% in August 2004, RA split into ASE and RO
@STRING{IEEE_J_RA         = "{IEEE} Transactions on Robotics and Automation"}
@STRING{IEEE_J_RBME       = "{IEEE} Reviews in Biomedical Engineering"}
@STRING{IEEE_J_RE         = "{IEEE} Transactions on Rehabilitation Engineering"}
@STRING{IEEE_J_RFI        = "{IEEE} Transactions on Radio Frequency Interference"}
@STRING{IEEE_J_RO         = "{IEEE} Transactions on Robotics"}
@STRING{IEEE_J_SAP        = "{IEEE} Transactions on Speech and Audio Processing"}
@STRING{IEEE_J_SC         = "{IEEE} Transactions on Services Computing"}
@STRING{IEEE_J_SE         = "{IEEE} Transactions on Software Engineering"}



% sensors
@STRING{IEEE_J_SENSOR     = "{IEEE} Sensors Journal"}
@STRING{IEEE_J_SM         = "{IEEE} Transactions on Semiconductor Manufacturing"}
@STRING{IEEE_J_SMC        = "{IEEE} Transactions on Systems, Man, and Cybernetics"}
@STRING{IEEE_J_SMCA       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {A}: Systems and Humans"}
@STRING{IEEE_J_SMCB       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {B}: Cybernetics"}
@STRING{IEEE_J_SMCC       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {C}: Applications and Reviews"}
@STRING{IEEE_J_SP         = "{IEEE} Transactions on Signal Processing"}



% circuits, signals, systems, audio and controls
@STRING{IEEE_J_SPL        = "{IEEE} Signal Processing Letters"}
@STRING{IEEE_J_SSC        = "{IEEE} Transactions on Systems Science and Cybernetics"}
@STRING{IEEE_J_STARS      = "{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing"}
@STRING{IEEE_J_STSP       = "{IEEE} Journal of Selected Topics in Signal Processing"}
@STRING{IEEE_J_SU         = "{IEEE} Transactions on Sonics and Ultrasonics"}
@STRING{IEEE_J_SYST       = "{IEEE} Systems Journal"}



% CAD
@STRING{IEEE_J_TCAD       = "{IEEE} Journal on Technology in Computer Aided Design"}
@STRING{IEEE_J_TJMJ       = "{IEEE} Translation Journal on Magnetics in Japan"}
@STRING{IEEE_J_UE         = "{IEEE} Transactions on Ultrasonics Engineering"}
@STRING{IEEE_J_UFFC       = "{IEEE} Transactions on Ultrasonics, Ferroelectrics, and Frequency Control"}
@STRING{IEEE_J_VC         = "{IEEE} Transactions on Vehicular Communications"}
@STRING{IEEE_J_VCG        = "{IEEE} Transactions on Visualization and Computer Graphics"}



% VLSI
@STRING{IEEE_J_VLSI       = "{IEEE} Transactions on Very Large Scale Integration ({VLSI}) Systems"}
@STRING{IEEE_J_VT         = "{IEEE} Transactions on Vehicular Technology"}
@STRING{IEEE_J_WCOM       = "{IEEE} Transactions on Wireless Communications"}






% IEEE Magazines



@STRING{IEEE_M_AES        = "{IEEE} Aerospace and Electronics Systems Magazine"}
@STRING{IEEE_M_AP         = "{IEEE} Antennas and Propagation Magazine"}
@STRING{IEEE_M_ASSP       = "{IEEE} {ASSP} Magazine"}
@STRING{IEEE_M_C          = "{IEEE} Computer"}
@STRING{IEEE_M_CAP        = "{IEEE} Computer Applications in Power"}
@STRING{IEEE_M_CAS        = "{IEEE} Circuits and Systems Magazine"}
@STRING{IEEE_M_CD         = "{IEEE} Circuits and Devices Magazine"}
@STRING{IEEE_M_CGA        = "{IEEE} Computer Graphics and Applications"}
@STRING{IEEE_M_CIM        = "{IEEE} Computational Intelligence Magazine"}
@STRING{IEEE_M_COM        = "{IEEE} Communications Magazine"}
@STRING{IEEE_M_COMSOC     = "{IEEE} Communications Society Magazine"}
@STRING{IEEE_M_CONC       = "{IEEE} Concurrency"}
@STRING{IEEE_M_CS         = "{IEEE} Control Systems Magazine"}
% CSEM changed to CSE in 1999
@STRING{IEEE_M_CSE        = "{IEEE} Computing in Science and Engineering"}
@STRING{IEEE_M_CSEM       = "{IEEE} Computational Science and Engineering Magazine"}
@STRING{IEEE_M_DTC        = "{IEEE} Design and Test of Computers"}
@STRING{IEEE_M_EI         = "{IEEE} Electrical Insulation Magazine"}
@STRING{IEEE_M_EMB        = "{IEEE} Engineering in Medicine and Biology Magazine"}
@STRING{IEEE_M_EMR        = "{IEEE} Engineering Management Review"}
@STRING{IEEE_M_ETR        = "{IEEE} ElectroTechnology Review"}
@STRING{IEEE_M_EXP        = "{IEEE} Expert"}
@STRING{IEEE_M_HIST       = "{IEEE} Annals of the History of Computing"}
@STRING{IEEE_M_IA         = "{IEEE} Industry Applications Magazine"}
@STRING{IEEE_M_IC         = "{IEEE} Internet Computing"}
@STRING{IEEE_M_IE         = "{IEEE} Industrial Electronics Magazine"}
@STRING{IEEE_M_IM         = "{IEEE} Instrumentation and Measurement Magazine"}
@STRING{IEEE_M_IS         = "{IEEE} Intelligent Systems"}
@STRING{IEEE_M_ITP        = "{IEEE} {IT} Professional"}
@STRING{IEEE_M_ITS        = "{IEEE} Intelligent Transportation Systems Magazine"}
@STRING{IEEE_M_MICRO      = "{IEEE} Micro"}
@STRING{IEEE_M_MM         = "{IEEE} Multimedia"}
@STRING{IEEE_M_MW         = "{IEEE} Microwave Magazine"}
@STRING{IEEE_M_NANO       = "{IEEE} Nanotechnology Magazine"}
@STRING{IEEE_M_NET        = "{IEEE} Network"}
@STRING{IEEE_M_PCOM       = "{IEEE} Personal Communications Magazine"}
% CAP and PER merged to form PE in 2003
@STRING{IEEE_M_PE         = "{IEEE} Power and Energy Magazine"}
@STRING{IEEE_M_PER        = "{IEEE} Power Engineering Review"}
@STRING{IEEE_M_POT        = "{IEEE} Potentials"}
@STRING{IEEE_M_PVC        = "{IEEE} Pervasive Computing"}
@STRING{IEEE_M_RA         = "{IEEE} Robotics and Automation Magazine"}
@STRING{IEEE_M_S          = "{IEEE} Software"}
@STRING{IEEE_M_SAP        = "{IEEE} Security and Privacy"}
@STRING{IEEE_M_SP         = "{IEEE} Signal Processing Magazine"}
@STRING{IEEE_M_SPECT      = "{IEEE} Spectrum"}
@STRING{IEEE_M_TODAY      = "Today's Engineer"}
@STRING{IEEE_M_TS         = "{IEEE} Technology and Society Magazine"}
@STRING{IEEE_M_VT         = "{IEEE} Vehicular Technology Magazine"}
@STRING{IEEE_M_WC         = "{IEEE} Wireless Communications Magazine"}






% IEEE Online Publications



@STRING{IEEE_O_CSTO        = "{IEEE} Communications Surveys and Tutorials"}
@STRING{IEEE_O_DSO         = "{IEEE} Distributed Systems Online"}


@STRING{J-AGU = {{EOS}, Transactions of the American Geophysical Union}}
@STRING{J-AI = {Artificial Intelligence}}
@STRING{J-AJA = {American Journal of Archaeology}}
@STRING{J-AMAI = {Annals of Mathematics and Artifical Intelligence}}
@STRING{J-AMS = {The Annals of Mathematical Statistics}}
@STRING{J-AR = {Autonomous Robots}}
@STRING{J-ASME-JBE = {Transactions of the ASME---Journal of Basic Engineering}}
@STRING{J-CSR = {Continental Shelf Research}}
@STRING{J-CVIU = {Computer Vision and Image Understanding}}
@STRING{J-DSR1 = {Deep Sea Research I}}
@STRING{J-EPSL = {Earth and Planetary Science Letters}}
@STRING{J-HI = {Hydro International}}
@STRING{J-IEEE = {Proceedings of the {IEEE}}}
@STRING{J-IJCV = {International Journal of Computer Vision}}
@STRING{J-IJME = {International Journal of Maritime Engineering}}
@STRING{J-IJNA = {International Journal of Nautical Archaeology}}
@STRING{J-IJOPE = {International Journal of Offshore and Polar Engineering}}
@STRING{J-IJPRAI = {International Journal of Pattern Recognition and Artifical Intelligence}}
@STRING{J-IJRR = {International Journal of Robotics Research}}
@STRING{J-IJSR = {International Journal of Systems Science}}
@STRING{J-IVC = {Image and Vision Computing}}
@STRING{J-JAIR = {Journal of Artificial Intelligence Research}}
@STRING{J-JAOT = {Journal of Atmospheric and Oceanic Technology}}
@STRING{J-JAS = {Journal of Applied Statistics}}
@STRING{J-JASA = {Journal of the American Statistical Association}}
@STRING{J-JASA2 = {Journal of the Acoustical Society of America}}
@STRING{J-JEME = {Journal of Engineering for the Maritime Environment}}
@STRING{J-JFA = {Journal of Field Archaeology}}
@STRING{J-JFR = {Journal of Field Robotics}}
@STRING{J-JGLR = {Journal of Great Lakes Research}}
@STRING{J-JGR = {Journal of Geophysical Research}}
@STRING{J-JIN = {Journal of the Institute of Navigation}}
@STRING{J-JIS = {Journal of Infrastructure Systems}}
@STRING{J-JMIV = {Journal of Mathematical Imaging and Vision}}
@STRING{J-JMLR = {Journal of Machine Learning Research}}
@STRING{J-JMS = {{ICES} Journal of Marine Science}}
@STRING{J-JMST = {Journal of Marine Science and Technology}}
@STRING{J-JOSA = {Journal of the Optical Society of America}}
@STRING{J-JOT = {Journal of Ocean Technology}}
@STRING{J-JPT = {Journal of Petroleum Technology}}
@STRING{J-JR = {Journal of Robotics}}
@STRING{J-LNCS = {Lecture Notes in Computer Science, Springer-Verlag}}
@STRING{J-MC = {Mathematics of Computation}}
@STRING{J-MEPS = {Marine Ecology Progress Series}}
@STRING{J-ML = {Machine Learning}}
@STRING{J-MTS = {Marine Technology Society Journal}}
@STRING{J-MVA = {Machine Vision and Applications}}
@STRING{J-NEJ = {Naval Engineers Journal}}
@STRING{J-NRR = {Naval Research Reviews}}
@STRING{J-OE = {Optical Engineering}}
@STRING{J-PNAS = {Proceedings of the National Academy of Sciences of the United States of America}}
@STRING{J-RAL = {{IEEE} Robotics and Automation Letters}}
@STRING{J-RAS = {Robotics and Autonomous Systems}}
@STRING{J-SP = {Signal Processing}}
@STRING{J-SSTA = {Subsurface Sensing Technologies and Applications}}
@STRING{J-ST = {Sea Technology}}
@STRING{NOTE-APPEAR = {{{Accepted, To Appear}}}}
@STRING{NOTE-CONDITIONAL = {{{Conditionally Accepted}}}}
@STRING{NOTE-PRESS = {{{In Press}}}}
@STRING{NOTE-SUBMITTED = {{{Submitted, Under Review}}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% conferences

@InProceedings{Dhanjal2019InProceedings,
  author    = {Sahib Singh Dhanjal and Maani Ghaffari and Ryan M. Eustice},
  booktitle = C-IROS,
  title     = {{DeepLocNet}: Deep Observation Classification and Ranging Bias Regression for Radio Positioning Systems},
  year      = {2019},
  address   = {Macau, China},
  month     = nov,
  pages     = {3802--3809},
  abstract  = {WiFi technology has been used pervasively in fine-grained indoor localization,
	gesture recognition, and adaptive communication. Achieving better performance in these 
	tasks generally boils down to differentiating Line-Of-Sight (LOS) from Non-Line-Of-Sight
	(NLOS) signal propagation reliably which generally requires expensive/specialized 
	hardware due to the complex nature of indoor environments. Hence, the development of
	low-cost accurate positioning systems that exploit available infrastructure is not
	entirely solved. In this paper, we develop a framework for indoor localization and
	tracking of ubiquitous mobile devices such as smartphones using on-board sensors.
	We present a novel deep LOS/NLOS classifier which uses the Received Signal Strength
	Indicator (RSSI), and can classify the input signal with an accuracy of 85%. The proposed
	algorithm can globally localize and track a smartphone (or robot) with a priori unknown
	location, and with a semi-accurate prior map (error within 0.8m) of the WiFi Access Points
	(AP). Through simultaneously solving for the trajectory and the map of access points, we 
	recover a trajectory of the device and corrected locations for the access points.
	Experimental evaluations of the framework show that localization accuracy is increased by 
	using the trained deep network; furthermore, the system becomes robust to any error in
	the map of APs.},
  groups    = {CURLY},
  url       = {https://arxiv.org/pdf/2002.00484.pdf},
}

@InProceedings{Hartley2018InProceedings,
  author    = {Ross Hartley and Maani Ghaffari Jadidi and Lu Gan and Jiunn-Kai Huang and Jessy W. Grizzle and Ryan M. Eustice},
  booktitle = C-IROS,
  title     = {Hybrid Contact Preintegration for Visual-Inertial-Contact State Estimation within Factor Graphs},
  year      = {2018},
  address   = {Madrid, Spain},
  month     = oct,
  pages     = {3783--3790},
  abstract  = {The factor graph framework is a convenient modeling technique for
	robotic state estimation where states are represented as nodes, and
	measurements are modeled as factors. When designing a sensor fusion
	framework for legged robots, one often has access to visual, inertial,
	joint encoder, and contact sensors. While visual-inertial odometry
	has been studied extensively in this framework, the addition of a
	preintegrated contact factor for legged robots has been only recently
	proposed. This allowed for integration of encoder and contact measurements
	into existing factor graphs, however, new nodes had to be added to
	the graph every time contact was made or broken. In this work, to
	cope with the problem of switching contact frames, we propose a hybrid
	contact preintegration theory that allows contact information to
	be integrated through an arbitrary number of contact switches. The
	proposed hybrid modeling approach reduces the number of required
	variables in the nonlinear optimization problem by only requiring
	new states to be added alongside camera or selected keyframes. This
	method is evaluated using real experimental data collected from a
	Cassie-series robot where the trajectory of the robot produced by
	a motion capture system is used as a proxy for ground truth. The
	evaluation shows that inclusion of the proposed preintegrated hybrid
	contact factor alongside visual-inertial navigation systems improves
	estimation accuracy as well as robustness to vision failure, while
	its generalization makes it more accessible for legged platforms.},
  groups    = {CURLY},
  url       = {https://arxiv.org/pdf/1803.07531.pdf},
}

@InProceedings{Ghaffari2019InProceedings,
  author    = {Maani Ghaffari and William Clark and Anthony Bloch and Ryan M. Eustice and Jessey W. Grizzle},
  booktitle = C-RSS,
  title     = {Continuous Direct Sparse Visual Odometry from {RGB-D} Images},
  year      = {2019},
  address   = {FreiburgimBreisgau, Germany},
  month     = jun,
  pages     = {1--9},
  abstract  = {This paper reports on a novel formulation and evaluation of visual 
	odometry from RGB-D images. Assuming a static scene, the developed theoretical
	framework generalizes the widely used direct energy formulation (photometric 
	error minimization) technique for obtaining a rigid body transformation that 
	aligns two overlapping RGB-D images to a continuous formulation. The continuity
	is achieved through functional treatment of the problem and representing the 
	process models over RGB-D images in a reproducing kernel Hilbert space; consequently,
	the registration is not limited to the specific image resolution and the framework
	is fully analytical with a closed-form derivation of the gradient. We solve the
	problem by maximizing the inner product between two functions defined over RGB-D
	images, while the continuous action of the rigid body motion Lie group is captured
	through the integration of the flow in the corresponding Lie algebra. Energy-based
	approaches have been extremely successful and the developed framework in this paper
	shares many of their desired properties such as the parallel structure on both CPUs
	and GPUs, sparsity, semi-dense tracking, avoiding explicit data association which is
	computationally expensive, and possible extensions to the simultaneous localization
	and mapping frameworks. The evaluations on experimental data and comparison with
	the equivalent energy-based formulation of the problem confirm the effectiveness
	of the proposed technique, especially, when the lack of structure and texture in
	the environment is evident.},
  groups    = {CURLY},
  url       = {https://arxiv.org/pdf/1904.02266.pdf},
}

@InProceedings{Hartley2018InProceedingsa,
  author    = {Ross Hartley and Maani Ghaffari Jadidi and Jessy W. Grizzle and Ryan M. Eustice},
  booktitle = C-RSS,
  title     = {Contact-Aided Invariant Extended Kalman Filtering for Legged Robot State Estimation},
  year      = {2018},
  address   = {Pittsburgh, PA, USA},
  month     = jun,
  pages     = {1--9},
  abstract  = {This paper derives a contact-aided inertial navigation observer for
	a 3D bipedal robot using the theory of invariant observer design.
	Aided inertial navigation is fundamentally a nonlinear observer design
	problem; thus, current solutions are based on approximations of the
	system dynamics, such as an Extended Kalman Filter (EKF), which uses
	a system's Jacobian linearization along the current best estimate
	of its trajectory. On the basis of the theory of invariant observer
	design by Barrau and Bonnabel, and in particular, the Invariant EKF
	(InEKF), we show that the error dynamics of the point contact-inertial
	system follows a log-linear autonomous differential equation; hence,
	the observable state variables can be rendered convergent with a
	domain of attraction that is independent of the system's trajectory.
	Due to the log-linear form of the error dynamics, it is not necessary
	to perform a nonlinear observability analysis to show that when using
	an Inertial Measurement Unit (IMU) and contact sensors, the absolute
	position of the robot and a rotation about the gravity vector (yaw)
	are unobservable. We further augment the state of the developed InEKF
	with IMU biases, as the online estimation of these parameters has
	a crucial impact on system performance. We evaluate theconvergence
	of the proposed system with the commonly used quaternion-based EKF
	observer using a Monte-Carlo simulation. In addition, our experimental
	evaluation using a Cassie-series bipedal robot shows that the contact-aided
	InEKF provides better performance in comparison with the quaternion-based
	EKF as a result of exploiting symmetries present in the system dynamics.},
  groups    = {CURLY},
  url       = {https://arxiv.org/pdf/1805.10410.pdf},
}

@InProceedings{Hartley2018InProceedingsb,
  author    = {Ross Hartley and Josh Mangelson and Lu Gan and Maani Ghaffari Jadidi and Jeffery M. Walls and Ryan M. Eustice and Jessy W. Grizzle},
  booktitle = C-ICRA,
  title     = {Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors},
  year      = {2018},
  address   = {Brisbane, Australia},
  month     = may,
  pages     = {4422--4429},
  abstract  = {State-of-the-art robotic perception systems have achieved sufficiently
	good performance using Inertial Measurement Units (IMUs), cameras,
	and nonlinear optimization techniques, that they are now being deployed
	as technologies. However, many of these methods rely significantly
	on vision and often fail when visual tracking is lost due to lighting
	or scarcity of features. This paper presents a state-estimation technique
	for legged robots that takes into account the robot's kinematic model
	as well as its contact with the environment. We introduce forward
	kinematic factors and preintegrated contact factors into a factor
	graph framework that can be incrementally solved in real-time. The
	forward kinematic factor relates the robot's base pose to a contact
	frame through noisy encoder measurements. The preintegrated contact
	factor provides odometry measurements of this contact frame while
	accounting for possible foot slippage. Together, the two developed
	factors constrain the graph optimization problem allowing the robot's
	trajectory to be estimated. The paper evaluates the method using
	simulated and real sensory IMU and kinematic data from experiments
	with a Cassie-series robot designed by Agility Robotics. These preliminary
	experiments show that using the proposed method in addition to IMU
	decreases drift and improves localization accuracy, suggesting that
	its use can enable successful recovery from a loss of visual tracking.},
  groups    = {CURLY},
  url       = {https://arxiv.org/pdf/1712.05873.pdf},
}

@InProceedings{GhaffariJadidi2018InProceedings,
  author       = {Ghaffari Jadidi, Maani and Patel, Mitesh and Miro, Jaime Valls and Dissanayake, Gamini and Biehl, Jacob and Girgensohn, Andreas},
  booktitle    = {Proceedings of the IEEE International Conference on Indoor Positioning and Indoor Navigation},
  title        = {A Radio-Inertial Localization and Tracking System with BLE Beacons Prior Maps},
  year         = {2018},
  organization = {IEEE},
  pages        = {1--8},
  abstract     = {In this paper, we develop a system for the low-cost indoor localization and tracking problem using radio signal strength indicator, Inertial Measurement Unit (IMU), and magnetometer sensors. We develop a novel and simplified probabilistic IMU motion model as the proposal distribution of the sequential Monte-Carlo technique to track the robot trajectory. Our algorithm can globally localize and track a robot with a priori unknown location, given an informative prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the problem as an optimization problem that serves as the Backend of the algorithm mentioned above (Front-end). Thus, by simultaneously solving for the robot trajectory and the map of BLE beacons, we recover a continuous and smooth trajectory of the robot, corrected locations of the BLE beacons, and the time-varying IMU bias. The evaluations achieved using hardware show that through the proposed closed-loop system the localization performance can be improved; furthermore, the system becomes robust to the error in the map of beacons by feeding back the optimized map to the Front-end.},
  url          = {https://arxiv.org/pdf/1706.05569.pdf},
}

@InProceedings{Parkison2018InProceedings,
  author    = {Steven A. Parkison and Lu Gan and Maani Ghaffari Jadidi and Ryan M. Eustice},
  booktitle = C-BMVC,
  title     = {Semantic Iterative Closest Point through Expectation-Maximization},
  year      = {2018},
  address   = {Newcastle, UK},
  month     = sep,
  pages     = {1--17},
  abstract  = {In this paper, we develop a novel point cloud registration algorithm
	that directly incorporates pixelated semantic measurements into the
	estimation of the relative transformation between two point clouds.
	The algorithm uses an Iterative Closest Point (ICP)-like scheme and
	performs joint semantic and geometric inference using the Expectation-Maximization
	technique in which semantic labels and point associations between
	two point clouds are treated as latent random variables. The minimization
	of the expected cost on the three-dimensional special Euclidean group,
	i.e., SE(3), yields the rigid body transformation between two point
	clouds. The evaluation on publicly available RGBD benchmarks shows
	that, in comparison with both the standard Generalized ICP (GICP)
	available in the Point Cloud Library and GICP on SE(3), the registration
	error is reduced.},
  groups    = {CURLY},
  url       = {http://141.212.194.179/publications/sparkison-2018a.pdf},
}

@InProceedings{GhaffariJadidi2014InProceedings,
  author       = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Valencia, Rafael and Andrade-Cetto, Juan},
  booktitle    = C-ICRA,
  title        = {Exploration on continuous Gaussian process frontier maps},
  year         = {2014},
  organization = {IEEE},
  pages        = {6077--6082},
  abstract     = {An information-driven autonomous robotic exploration method on a continuous representation of unknown environments is proposed in this paper. The approach conveniently handles sparse sensor measurements to build a continuous model of the environment that exploits structural dependencies without the need to resort to a fixed resolution grid map. A gradient field of occupancy probability distribution is regressed from sensor data as a Gaussian process providing frontier boundaries for further exploration. The resulting continuous global frontier surface completely describes unexplored regions and, inherently, provides an automatic stop criterion for a desired sensitivity. The performance of the proposed approach is evaluated through simulation results in the well-known Freiburg and Cave maps.},
  url          = {https://upcommons.upc.edu/bitstream/handle/2117/28286/1497-Exploration-on-continuous-Gaussian-process-frontier-maps.pdf%3Bjsessionid%3DE2AD0F2DA15B663C329548679C443986?sequence%3D1},
}

@InProceedings{GhaffariJadidi2015InProceedings,
  author       = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Dissanayake, Gamini},
  booktitle    = C-IROS,
  title        = {Mutual information-based exploration on continuous occupancy maps},
  year         = {2015},
  organization = {IEEE},
  pages        = {6086--6092},
  abstract     = {The problem of active perception with an autonomous robot is studied in this paper. It is proposed that the exploratory behavior of the robot be controlled using mutual information (MI) surfaces between the current map and a one-step look ahead measurements. MI surfaces highlight informative areas for exploration. A novel method for computing these surfaces is described. An approach that exploits structural dependencies of the environment and handles sparse sensor measurements to build a continuous model of the environment, that can then be used to generate MI surfaces is also proposed. A gradient field of occupancy probability distribution is regressed from sensor data as a Gaussian Process and provide frontier boundaries for further exploration. The continuous global frontier surface completely describes unexplored regions and, inherently, provides an automatic termination criterion for a desired sensitivity. The results from publicly available datasets confirm an average improvement of the proposed methodology over comparable standard and state-of-the-art exploratory methods available in the literature by more than 20% and 13% in travel distance and map entropy reduction rate, respectively.},
  url          = {https://opus.lib.uts.edu.au/bitstream/10453/39971/6/Mutual%20Information-based%20Exploration.pdf},
}

@InProceedings{GhaffariJadidi2017InProceedings,
  author       = {Ghaffari Jadidi, Maani and Patel, Mitesh and Valls Miro, Jaime},
  booktitle    = C-ICRA,
  title        = {Gaussian processes online observation classification for RSSI-based low-cost indoor positioning systems},
  year         = {2017},
  organization = {IEEE},
  pages        = {6269--6275},
  abstract     = {In this paper, we propose a real-time classification scheme to cope with noisy Radio Signal Strength Indicator (RSSI) measurements utilized in indoor positioning systems. RSSI values are often converted to distances for position estimation. However due to multipathing and shadowing effects, finding a unique sensor model using both parametric and non-parametric methods is highly challenging. We learn decision regions using the Gaussian Processes classification to accept measurements that are consistent with the operating sensor model. The proposed approach can perform online, does not rely on a particular sensor model or parameters, and is robust to sensor failures. The experimental results achieved using hardware show that available positioning algorithms can benefit from incorporating the classifier into their measurement model as a meta-sensor modeling technique.},
  url          = {https://arxiv.org/pdf/1609.03130.pdf},
}

@InProceedings{Emery2016InProceedings,
  author    = {Emery, Brendan M. and Ghaffari Jadidi, Maani and Nakamura, Keisuke and Valls Miro, Jaime},
  booktitle = {Proceedings of the Australasian Conference on Robotics and Automation},
  title     = {An audio-visual solution to sound source localization and tracking with applications to HRI},
  year      = {2016},
  pages     = {1--10},
  abstract  = {Robot audition is an emerging and growing branch in the robotic community and is necessary for a natural Human-Robot Interaction (HRI). In this paper, we propose a framework that integrates advances from Simultaneous Localization And Mapping (SLAM), bearing-only target tracking, and robot audition techniques into a unifed system for sound source identification, localization, and tracking. In indoors, acoustic observations are often highly noisy and corrupted due to reverberations, the robot ego-motion and background noise, and possible discontinuous nature of them. Therefore, in everyday interaction scenarios, the system requires accommodating for outliers, robust data association, and appropriate management of the landmarks, i.e. sound sources. We solve the robot self-localization and environment representation problems using an RGB-D SLAM algorithm, and sound source localization and tracking using recursive Bayesian estimation in the form of the extended Kalman Filter with unknown data associations and an unknown number of landmarks. The experimental results show that the proposed system performs well in the medium-sized cluttered indoor environment.},
  url       = {https://opus.lib.uts.edu.au/bitstream/10453/130936/1/ACRA16.pdf},
}

@InProceedings{Hashemi2009InProceedings,
  author       = {Hashemi, Ehsan and Ghaffari Jadidi, Maani and Babarsad, Omid Bakhshandeh},
  booktitle    = {IEEE International Symposium on Computational Intelligence in Robotics and Automation},
  title        = {Trajectory planning optimization with dynamic modeling of four wheeled omni-directional mobile robots},
  year         = {2009},
  organization = {IEEE},
  pages        = {272--277},
  abstract     = {Path planning together with the tuning and determination of controller parameters are major concerns in omnidirectional mobile robots. Defining appropriate controller parameters in acceleration and deceleration to reach far and near target points without slippage is one of critical issues since some troubles due to unregulated velocities may greatly affect the ability of robot for the specified path planning and attaining the mentioned targets. A robot accurate kinematic and dynamic modeling and simulation accompanied by velocity and acceleration filtering are mainly discussed in this paper. Major changes and improvements in motion analysis, simulation and accuracy for the newly presented model and its efficiency are discussed in comparison with the previous simple kinematic modeling. Employing the new approach for robot dynamic modeling, particularly acceleration filtering, results in to the more precise robot control and achieving appropriate results.},
  doi          = {https://doi.org/10.1109/CIRA.2009.5423195},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% workshops

@InProceedings{Jadidi2017InProceedings,
  author    = {Maani Ghaffari Jadidi and Lu Gan and Steven A. Parkison and Jie Li and Ryan M. Eustice},
  booktitle = {RSS Workshop on Spatial-Semantic Representations in Robotics},
  title     = {Gaussian processes semantic map representation},
  year      = {2017},
  address   = {Cambridge, MA, USA},
  month     = jul,
  abstract  = {In this paper, we develop a high-dimensional map building technique
	that incorporates raw pixelated semantic measurements into the map
	representation. The proposed technique uses Gaussian Processes (GPs)
	multi-class classification for map inference and is the natural extension
	of GP occupancy maps from binary to multi-class form. The technique
	exploits the continuous property of GPs and, as a result, the map
	can be inferred with any resolution. In addition, the proposed GP
	Semantic Map (GPSM) learns the structural and semantic correlation
	from measurements rather than resorting to assumptions, and can flexibly
	learn the spatial correlation as well as any additional non-spatial
	correlation between map points. We extend the OctoMap to Semantic
	OctoMap representation and compare with the GPSM mapping performance
	using NYU Depth V2 dataset. Evaluations of the proposed technique
	on multiple partially labeled RGBD scans and labels from noisy image
	segmentation show that the GP semantic map can handle sparse measurements,
	missing labels in the point cloud, as well as noise corrupted labels.},
  groups    = {CURLY},
  url       = {https://arxiv.org/pdf/1707.01532},
}

@InProceedings{GhaffariJadidi2013InProceedings,
  author    = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Valencia Carreno, Rafael and Andrade-Cetto, Juan and Dissanayake, Gamini},
  booktitle = {RSS Workshop on Robotic Exploration, Monitoring, and Information Collection},
  title     = {Exploration in Information Distribution Maps},
  year      = {2013},
  address   = {Berlin, Germany},
  pages     = {1--8},
  abstract  = {In this paper, a novel solution for autonomous robotic exploration is proposed. We model the distribution of information in an unknown environment as an unsteady diffusion process, which can be an appropriate mathematical formulation and analogy for expanding, time-varying, and dynamic envi-ronments. This information distribution map is the solution of the diffusion process partial differential equation, and is regressed from sensor data as a Gaussian Process. Optimization of the process parameters leads to an optimal frontier map which describes regions of interest for further exploration. Since the presented approach considers a continuous model of the environment, it can be used to plan smooth exploration paths exploiting the structural dependencies of the environment whilst handling sparse sensor measurements. The performance of the approach is evaluated through simulation results in the well-known Freiburg and Cave maps.},
  url       = {http://www.iri.upc.edu/files/scidoc/1428-Exploration-in-Information-Distribution-Maps.pdf},
}

@InProceedings{Zhu2020InProceedings,
  author    = {Zhu, Minghan and Ghaffari, Maani and Zhong, Yuanxin and Lu, Pingping and Cao, Zhong and Eustice, Ryan M. and Peng, Huei},
  booktitle = C-IROS,
  title     = {Monocular Depth Prediction through Continuous 3D Loss},
  year      = {2020},
  pages     = {10742-10749},
  abstract  = {This paper reports a new continuous 3D loss function for learning depth from monocular images. The dense depth prediction from a monocular image is supervised using sparse LIDAR points, which enables us to leverage available open source datasets with camera-LIDAR sensor suites during training. Currently, accurate and affordable range sensor is not readily available. Stereo cameras and LIDARs measure depth either inaccurately or sparsely/costly. In contrast to the current point-to-point loss evaluation approach, the proposed 3D loss treats point clouds as continuous objects; therefore, it compensates for the lack of dense ground truth depth due to LIDAR's sparsity measurements. We applied the proposed loss in three state-of-the-art monocular depth prediction approaches DORN, BTS, and Monodepth2. Experimental evaluation shows that the proposed loss improves the depth prediction accuracy and produces point-clouds with more consistent 3D geometric structures compared with all tested baselines, implying the benefit of the proposed loss on general depth prediction networks. A video demo of this work is available at https://youtu.be/5HL8BjSAY4Y.},
  doi       = {10.1109/IROS45743.2020.9341767},
  url       = {https://arxiv.org/abs/2003.09763},
}

@InProceedings{Zhang2021InProceedings,
  author    = {Zhang, Ray and Lin, Tzu-Yuan and Lin, Chien Erh and Parkison, Steven A. and Clark, William and Grizzle, Jessy W. and Eustice, Ryan M. and Ghaffari, Maani},
  booktitle = C-ICRA,
  title     = {A New Framework for Registration of Semantic Point Clouds from Stereo and RGB-D Cameras},
  year      = {2021},
  pages     = {12214-12221},
  abstract  = {This paper reports on a novel nonparametric rigid point cloud registration framework, Semantic Continuous Visual Odometry (CVO), that jointly integrates geometric and semantic measurements such as color or semantic labels into the alignment process and does not require explicit data association. The point clouds are represented as nonparametric functions in a reproducible kernel Hilbert space. The alignment problem is formulated as maximizing the inner product between two functions, essentially a sum of weighted kernels, each of which exploits the local geometric and semantic features. As a result of the continuous models, analytical gradients can be computed, and a local solution can be obtained by optimization over the rigid body transformation group. Besides, we present a new point cloud alignment metric that is intrinsic to the proposed framework and takes into account geometric and semantic information. The evaluations using publicly available stereo and RGB-D datasets show that the proposed method outperforms state-of-the-art outdoor and indoor frame-to-frame registration methods. An open-source GPU implementation is also provided.},
  doi       = {10.1109/ICRA48506.2021.9561929},
  url       = {https://arxiv.org/abs/2012.03683},
}

@InProceedings{Lin2022InProceedings,
  author    = {Lin, Tzu-Yuan and Zhang, Ray and Yu, Justin and Ghaffari, Maani},
  booktitle = {Proceedings of the 5th Conference on Robot Learning},
  title     = {Legged Robot State Estimation using Invariant Kalman Filtering and Learned Contact Events},
  year      = {2022},
  editor    = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  month     = {08--11 Nov},
  pages     = {1057--1066},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {164},
  abstract  = {This work develops a learning-based contact estimator for legged robots that bypasses the need for physical sensors and takes multi-modal proprioceptive sensory data as input. Unlike vision-based state estimators, proprioceptive state estimators are agnostic to perceptually degraded situations such as dark or foggy scenes. While some robots are equipped with dedicated physical sensors to detect necessary contact data for state estimation, some robots do not have dedicated contact sensors, and the addition of such sensors is non-trivial without redesigning the hardware. The trained network can estimate contact events on different terrains. The experiments show that a contact-aided invariant extended Kalman filter can generate accurate odometry trajectories compared to a state-of-the-art visual SLAM system, enabling robust proprioceptive odometry.},
  pdf       = {https://proceedings.mlr.press/v164/lin22b/lin22b.pdf},
  url       = {https://proceedings.mlr.press/v164/lin22b.html},
}

@InProceedings{Song2022InProceedings,
  author    = {Song, Jingwei and Zhu, Qiuchen and Lin, Jianyu and Ghaffari, Maani},
  booktitle = {Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022},
  title     = {Bayesian Dense Inverse Searching Algorithm forReal-Time Stereo Matching inMinimally Invasive Surgery},
  year      = {2022},
  address   = {Cham},
  editor    = {Wang, Linwei and Dou, Qi and Fletcher, P. Thomas and Speidel, Stefanie and Li, Shuo},
  pages     = {333--344},
  publisher = {Springer Nature Switzerland},
  abstract  = {This paper reports a CPU-level real-time stereo matching method for surgical images (10Hz on {\$}{\$}640 {\backslash}times 480{\$}{\$}640{\texttimes}480image with a single core of i5-9400). The proposed method is built on the fast LK algorithm, which estimates the disparity of the stereo images patch-wisely and in a coarse-to-fine manner. We propose a Bayesian framework to evaluate the probability of the optimized patch disparity at different scales. Moreover, we introduce a spatial Gaussian mixed probability distribution to address the pixel-wise probability within the patch. In-vivo and synthetic experiments show that our method can handle ambiguities resulted from the textureless surfaces and the photometric inconsistency caused by the non-Lambertian reflectance. Our Bayesian method correctly balances the probability of the patch for stereo images at different scales. Experiments indicate that the estimated depth has similar accuracy and fewer outliers than the baseline methods in the surgical scenario with real-time performance. The code and data set are available at https://github.com/JingweiSong/BDIS.git.},
  isbn      = {978-3-031-16449-1},
  url       = {https://arxiv.org/abs/2106.07136},
}

@InProceedings{Zhu2022InProceedings,
  author    = {Zhu, Minghan and Ghaffari, Maani and Peng, Huei},
  booktitle = {Proceedings of the 5th Conference on Robot Learning},
  title     = {Correspondence-Free Point Cloud Registration with SO(3)-Equivariant Implicit Shape Representations},
  year      = {2022},
  editor    = {Faust, Aleksandra and Hsu, David and Neumann, Gerhard},
  month     = {08--11 Nov},
  pages     = {1412--1422},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {164},
  abstract  = {This paper proposes a correspondence-free method for point cloud rotational registration. We learn an embedding for each point cloud in a feature space that preserves the SO(3)-equivariance property, enabled by recent developments in equivariant neural networks. The proposed shape registration method achieves three major advantages through combining equivariant feature learning with implicit shape models. First, the necessity of data association is removed because of the permutation-invariant property in network architectures similar to PointNet. Second, the registration in feature space can be solved in closed-form using Horns method due to the SO(3)-equivariance property. Third, the registration is robust to noise in the point cloud because of the joint training of registration and implicit shape reconstruction. The experimental results show superior performance compared with existing correspondence-free deep registration methods.},
  pdf       = {https://proceedings.mlr.press/v164/zhu22b/zhu22b.pdf},
  url       = {https://proceedings.mlr.press/v164/zhu22b.html},
}

@InProceedings{Kathuria2022InProceedings,
  author    = {Kathuria, Tribhi and Xu, Yifan and Chakhachiro, Theodor and Yang, X Jessie and Ghaffari, Maani},
  booktitle = {IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  title     = {Providers-Clients-Robots: Framework for spatial-semantic planning for shared understanding in human-robot interaction},
  year      = {2022},
  pages     = {1-8},
  abstract  = {This paper develops a novel framework called Providers-Clients-Robots (PCR), applicable to socially assistive robots that support research on shared understanding in human-robot interactions. Providers, Clients, and Robots share an actionable and intuitive representation of the environment to create plans that best satisfy the combined needs of all parties. The plans are formed via interaction between the Client and the Robot based on a previously built multi-modal navigation graph. The explainable environmental representation in the form of a navigation graph is constructed collaboratively between Providers and Robots prior to interaction with Clients. We develop a realization of the proposed framework to create a spatial-semantic representation of an indoor environment autonomously. Moreover, we develop a planner that takes in constraints from Providers and Clients of the establishment and dynamically plans a sequence of visits to each area of interest. Evaluations show that the proposed realization of the PCR framework can successfully make plans while satisfying the specified time budget and sequence constraints and outperforming the greedy baseline.},
  url       = {https://arxiv.org/abs/2206.10767},
}

@InProceedings{Teng2022InProceedings,
  author    = {Teng, Sangli and Sanyal, Amit K. and Vasudevan, Ram and Bloch, Anthony and Ghaffari, Maani},
  booktitle = C-ACC,
  title     = {Input Influence Matrix Design for MIMO Discrete-Time Ultra-Local Model},
  year      = {2022},
  pages     = {2730-2735},
  abstract  = {Ultra-Local Models (ULM) have been applied to perform model-free control of nonlinear systems with unknown or partially known dynamics. Unfortunately, extending these methods to MIMO systems requires designing a dense input influence matrix which is challenging. This paper presents guidelines for designing an input influence matrix for discretetime, control-affine MIMO systems using an ULM-based controller. This paper analyzes the case that uses ULM and a model-free control scheme: the Hlder-continuous Finite-Time Stable (FTS) control. By comparing the ULM with the actual system dynamics, the paper describes how to extract the input-dependent part from the lumped ULM dynamics and finds that the tracking and state estimation error are coupled. The stability of the ULM-FTS error dynamics is affected by the eigenvalues of the difference (defined by matrix multiplication) between the actual and designed input influence matrix. Finally, the paper shows that a wide range of input influence matrix designs can keep the ULM-FTS error dynamics (at least locally) asymptotically stable. A numerical simulation is included to verify the result. The analysis can also be extended to other ULM-based controllers.},
  doi       = {10.23919/ACC53348.2022.9867513},
  url       = {https://arxiv.org/abs/2203.08729},
}

@InProceedings{Teng2022InProceedingsa,
  author    = {Teng, Sangli and Chen, Dianhao and Clark, William and Ghaffari, Maani},
  booktitle = C-IROS,
  title     = {An Error-State Model Predictive Control on Connected Matrix Lie Groups for Legged Robot Control},
  year      = {2022},
  pages     = {1-8},
  abstract  = {This paper reports on a new error-state Model Predictive Control (MPC) approach to connected matrix Lie groups for robot control. The linearized tracking error dynamics and the linearized equations of motion are derived in the Lie algebra. Moreover, given an initial condition, the linearized tracking error dynamics and equations of motion are globally valid and evolve independently of the system trajectory. By exploiting the symmetry of the problem, the proposed approach shows faster convergence of rotation and position simultaneously than the state-of-the-art geometric variational MPC based on variational-based linearization. Numerical simulation on tracking control of a fully-actuated 3D rigid body dynamics confirms the benefits of the proposed approach compared to the baselines. Furthermore, the proposed MPC is also verified in pose control and locomotion experiments on a quadrupedal robot MIT Mini Cheetah.},
  url       = {https://arxiv.org/abs/2203.08728},
}

@InProceedings{Teng2022InProceedingsb,
  author    = {Teng, Sangli and Clark, William and Bloch, Anthony and Vasudevan, Ram and Ghaffari, Maani},
  booktitle = C-CDC,
  title     = {Lie Algebraic Cost Function Design for Control on Lie Groups},
  year      = {2022},
  pages     = {1-8},
  abstract  = {This paper presents a control framework on Lie groups by designing the control objective in its Lie algebra. Control on Lie groups is challenging due to its nonlinear nature and difficulties in system parameterization. Existing methods to design the control objective on a Lie group and then derive the gradient for controller design are non-trivial and can result in slow convergence in tracking control. We show that with a proper left-invariant metric, setting the gradient of the cost function as the tracking error in the Lie algebra leads to a quadratic Lyapunov function that enables globally exponential convergence. In the PD control case, we show that our controller can maintain an exponential convergence rate even when the initial error is approaching $\pi$ in SO(3). We also show the merit of this proposed framework in trajectory optimization. The proposed cost function enables the iterative Linear Quadratic Regulator (iLQR) to converge much faster than the Differential Dynamic Programming (DDP) with a well-adopted cost function when the initial trajectory is poorly initialized on SO(3).},
  url       = {https://arxiv.org/abs/2204.09177},
}

@Comment{jabref-meta: databaseType:bibtex;}
