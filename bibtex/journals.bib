%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@STRING{C-AAAI = {Proceedings of the {AAAI} National Conference on Artificial Intelligence}}
@STRING{C-ACC = {Proceedings of the American Control Conference}}
@STRING{C-AGU = {EOS: Transactions of the American Geophysical Union Fall Meeting Supplement}}
@STRING{C-AUV = {Proceedings of the {IEEE/OES} Autonomous Underwater Vehicles Conference}}
@STRING{C-BMVC = {Proceedings of the British Machine Vision Conference}}
@STRING{C-CDC = {Proceedings of the {IEEE} Conference on Decision and Control}}
@STRING{C-CORL = {Proceedings of the Conference on Robot Learning}}
@STRING{C-CVPR = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition}}
@STRING{C-DICTA = {Proceedings of Digital Image Computing: Techniques and Applications}}
@STRING{C-ECCV = {Proceedings of the European Conference on Computer Vision}}
@STRING{C-ECMR = {Proceedings of the European Conference on Mobile Robotics}}
@STRING{C-FSR = {Proceedings of the International Conference on Field and Service Robotics}}
@STRING{C-ICCV = {Proceedings of the {IEEE} International Conference on Computer Vision}}
@STRING{C-ICIP = {Proceedings of the International Conference on Image Processing}}
@STRING{C-ICPR = {Proceedings of the International Conference Pattern Recognition}}
@STRING{C-ICRA = {Proceedings of the {IEEE} International Conference on Robotics and Automation}}
@STRING{C-IJCAI = {Proceedings of the International Joint Conference on Artifical Intelligence}}
@STRING{C-IROS = {Proceedings of the {IEEE}/{RSJ} International Conference on Intelligent Robots and Systems}}
@STRING{C-ISCV = {Proceedings of the International Symposium on Computer Vision}}
@STRING{C-ISER = {Proceedings of the International Symposium on Experimental Robotics}}
@STRING{C-ISRR = {Proceedings of the International Symposium on Robotics Research}}
@STRING{C-ISUT = {Proceedings of the International Symposium on Underwater Technology}}
@STRING{C-IVS  = {Proceedings of the {IEEE} Intelligent Vehicle Symposium}}
@STRING{C-NIPS = {Proceedings of the Advances in Neural Information Processing Systems Conference}}
@STRING{C-OCEANS = {Proceedings of the {IEEE}/{MTS} {OCEANS} Conference and Exhibition}}
@STRING{C-OCEANS-Europe = {Proceedings of the {IEEE} {OCEANS}-Europe Conference and Exhibition}}
@STRING{C-RSS = {Proceedings of the Robotics: Science and Systems Conference}}
@STRING{C-UAI = {Proceedings of Uncertainty in AI}}
@STRING{C-UUST = {Proceedings of the International Symposium on Unmanned Untethered Submersible Technology}}
@STRING{C-WAFR = {Proceedings of the International Workshop on the Algorithmic Foundations of Robotics}}
@STRING{C-WUNET = {Proceedings of the ACM International Conference on Underwater Networks and Systems (WUWNet)}}


@STRING{I-CALTECH = {California Institue of Technology}}
@STRING{I-CMU = {Carnegie Mellon University}}
@STRING{I-JHU = {Johns Hopkins University}}
@STRING{I-KTH = {Katholieke Universiteit Leuven}}
@STRING{I-MIT = {Massachusetts Institute of Technology}}
@STRING{I-MIT/WHOI = {Massachusetts Institute of Technology / Woods Hole Oceanographic Institution Joint Program}}
@STRING{I-SCRIPPS = {Scripps Institution of Oceanography}}
@STRING{I-UMICH = {University of Michigan}}
@STRING{I-WHOI = {Woods Hole Oceanographic Institution}}
@STRING{IEEE_J_AC         = "{IEEE} Transactions on Automatic Control"}



% components, packaging and manufacturing
@STRING{IEEE_J_ADVP       = "{IEEE} Transactions on Advanced Packaging"}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% IEEE Journals 



% aerospace and military
@STRING{IEEE_J_AES        = "{IEEE} Transactions on Aerospace and Electronic Systems"}
@STRING{IEEE_J_AIRE       = "{IEEE} Transactions on Airborne Electronics"}
@STRING{IEEE_J_ANE        = "{IEEE} Transactions on Aerospace and Navigational Electronics"}
@STRING{IEEE_J_ANNE       = "{IEEE} Transactions on Aeronautical and Navigational Electronics"}
@STRING{IEEE_J_AP         = "{IEEE} Transactions on Antennas and Propagation"}



% industrial, commercial and consumer
@STRING{IEEE_J_APPIND     = "{IEEE} Transactions on Applications and Industry"}
@STRING{IEEE_J_AS         = "{IEEE} Transactions on Aerospace"}
@STRING{IEEE_J_ASC        = "{IEEE} Transactions on Applied Superconductivity"}



% cybernetics, ergonomics, robots, man-machine, and automation
@STRING{IEEE_J_ASE        = "{IEEE} Transactions on Automation Science and Engineering"}
@STRING{IEEE_J_ASSP       = "{IEEE} Transactions on Acoustics, Speech, and Signal Processing"}
@STRING{IEEE_J_AU         = "{IEEE} Transactions on Audio"}
@STRING{IEEE_J_AUEA       = "{IEEE} Transactions on Audio and Electroacoustics"}



% electromagnetics, antennas, EMI, magnetics and microwave
@STRING{IEEE_J_AWPL       = "{IEEE} Antennas and Wireless Propagation Letters"}
% Note: The B-ME journal later dropped the hyphen and became the BME.
@STRING{IEEE_J_B-ME       = "{IEEE} Transactions on Bio-Medical Engineering"}
@STRING{IEEE_J_BC         = "{IEEE} Transactions on Broadcasting"}



% medical and biological
@STRING{IEEE_J_BCAS       = "{IEEE} Transactions on Biomedical Circuits and Systems"}
@STRING{IEEE_J_BCTV       = "{IEEE} Transactions on Broadcast and Television Receivers"}
@STRING{IEEE_J_BME        = "{IEEE} Transactions on Biomedical Engineering"}
@STRING{IEEE_J_BMELC      = "{IEEE} Transactions on Bio-Medical Electronics"}



% computers, computation, networking and software
@STRING{IEEE_J_C          = "{IEEE} Transactions on Computers"}
@STRING{IEEE_J_CAD        = "{IEEE} Transactions on Computer-Aided Design of Integrated Circuits and Systems"}
@STRING{IEEE_J_CAL        = "{IEEE} Computer Architecture Letters"}
@STRING{IEEE_J_CAPT       = "{IEEE} Transactions on Components and Packaging Technology"}
@STRING{IEEE_J_CAPTS      = "{IEEE} Transactions on Components and Packaging Technologies"}
@STRING{IEEE_J_CAS        = "{IEEE} Transactions on Circuits and Systems"}
@STRING{IEEE_J_CASI       = "{IEEE} Transactions on Circuits and Systems---Part {I}: Fundamental Theory and Applications"}
% in 2004 CASI and CASII renamed part title to CASI_RP and CASII_EB, respectively.
@STRING{IEEE_J_CASI_RP    = "{IEEE} Transactions on Circuits and Systems---Part {I}: Regular Papers"}
@STRING{IEEE_J_CASII      = "{IEEE} Transactions on Circuits and Systems---Part {II}: Analog and Digital Signal Processing"}
@STRING{IEEE_J_CASII_EB   = "{IEEE} Transactions on Circuits and Systems---Part {II}: Express Briefs"}
@STRING{IEEE_J_CASVT      = "{IEEE} Transactions on Circuits and Systems for Video Technology"}
@STRING{IEEE_J_CBB        = "{IEEE/ACM} Transactions on Computational Biology and Bioinformatics"}
@STRING{IEEE_J_CE         = "{IEEE} Transactions on Consumer Electronics"}
@STRING{IEEE_J_CHMT       = "{IEEE} Transactions on Components, Hybrids and Manufacturing Technology"}



% education, engineering, history, IEEE, professional
@STRING{IEEE_J_CJECE      = "Canadian Journal of Electrical and Computer Engineering"}
@STRING{IEEE_J_COM        = "{IEEE} Transactions on Communications"}



% communications
@STRING{IEEE_J_COML       = "{IEEE} Communications Letters"}
@STRING{IEEE_J_COMT       = "{IEEE} Transactions on Communication Technology"}
@STRING{IEEE_J_CPART      = "{IEEE} Transactions on Component Parts"}
@STRING{IEEE_J_CPMTA      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {A}"}
@STRING{IEEE_J_CPMTB      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {B}: Advanced Packaging"}
@STRING{IEEE_J_CPMTC      = "{IEEE} Transactions on Components, Packaging and Manufacturing Technology---Part {C}: Manufacturing"}
@STRING{IEEE_J_CST        = "{IEEE} Transactions on Control Systems Technology"}
@STRING{IEEE_J_CT         = "{IEEE} Transactions on Circuit Theory"}
@STRING{IEEE_J_DEI        = "{IEEE} Transactions on Dielectrics and Electrical Insulation"}



% reliability
@STRING{IEEE_J_DMR        = "{IEEE} Transactions on Device and Materials Reliability"}
@STRING{IEEE_J_DSC        = "{IEEE} Transactions on Dependable and Secure Computing"}



% energy and power
@STRING{IEEE_J_EC         = "{IEEE} Transactions on Energy Conversion"}
@STRING{IEEE_J_ECOMP      = "{IEEE} Transactions on Electronic Computers"}
@STRING{IEEE_J_ED         = "{IEEE} Transactions on Electron Devices"}



% physics, electrons, nanotechnology, nuclear and quantum electronics
@STRING{IEEE_J_EDL        = "{IEEE} Electron Device Letters"}
@STRING{IEEE_J_EDU        = "{IEEE} Transactions on Education"}
@STRING{IEEE_J_EI         = "{IEEE} Transactions on Electrical Insulation"}
@STRING{IEEE_J_EM         = "{IEEE} Transactions on Engineering Management"}
@STRING{IEEE_J_EMC        = "{IEEE} Transactions on Electromagnetic Compatibility"}
@STRING{IEEE_J_EPM        = "{IEEE} Transactions on Electronics Packaging Manufacturing"}



% semiconductors, superconductors, electrochemical and solid state
@STRING{IEEE_J_ESSL       = "{IEEE/ECS} Electrochemical and Solid-State Letters"}
@STRING{IEEE_J_EVC        = "{IEEE} Transactions on Evolutionary Computation"}
@STRING{IEEE_J_EWS        = "{IEEE} Transactions on Engineering Writing and Speech"}
@STRING{IEEE_J_FUZZ       = "{IEEE} Transactions on Fuzzy Systems"}



% earth, wind, fire and water
@STRING{IEEE_J_GE         = "{IEEE} Transactions on Geoscience Electronics"}
@STRING{IEEE_J_GRS        = "{IEEE} Transactions on Geoscience and Remote Sensing"}
@STRING{IEEE_J_GRSL       = "{IEEE} Geoscience and Remote Sensing Letters"}
@STRING{IEEE_J_H          = "{IEEE} Transactions on Haptics"}
@STRING{IEEE_J_HFE        = "{IEEE} Transactions on Human Factors in Electronics"}
@STRING{IEEE_J_IA         = "{IEEE} Transactions on Industry Applications"}
@STRING{IEEE_J_IE         = "{IEEE} Transactions on Industrial Electronics"}
@STRING{IEEE_J_IECI       = "{IEEE} Transactions on Industrial Electronics and Control Instrumentation"}
@STRING{IEEE_J_IFS        = "{IEEE} Transactions on Information Forensics and Security"}
@STRING{IEEE_J_IGA        = "{IEEE} Transactions on Industry and General Applications"}
@STRING{IEEE_J_IINF       = "{IEEE} Transactions on Industrial Informatics"}



% instrumentation and measurement
@STRING{IEEE_J_IM         = "{IEEE} Transactions on Instrumentation and Measurement"}
@STRING{IEEE_J_IP         = "{IEEE} Transactions on Image Processing"}



% coding, data, information, knowledge
@STRING{IEEE_J_IT         = "{IEEE} Transactions on Information Theory"}
@STRING{IEEE_J_ITBM       = "{IEEE} Transactions on Information Technology in Biomedicine"}



% autos, transportation and vehicles (non-aerospace)
@STRING{IEEE_J_ITS        = "{IEEE} Transactions on Intelligent Transportation Systems"}



% computer graphics, imaging, and multimedia
@STRING{IEEE_J_JDT        = "{IEEE/OSA} Journal of Display Technology"}



% insulation and materials
@STRING{IEEE_J_JEM        = "{IEEE/TMS} Journal of Electronic Materials"}
@STRING{IEEE_J_JLT        = "{IEEE/OSA} Journal of Lightwave Technology"}
@STRING{IEEE_J_JQE        = "{IEEE} Journal of Quantum Electronics"}
@STRING{IEEE_J_JRA        = "{IEEE} Journal of Robotics and Automation"}
@STRING{IEEE_J_JSAC       = "{IEEE} Journal on Selected Areas in Communications"}
@STRING{IEEE_J_JSSC       = "{IEEE} Journal of Solid-State Circuits"}
@STRING{IEEE_J_JSTQE      = "{IEEE} Journal of Selected Topics in Quantum Electronics"}
@STRING{IEEE_J_KDE        = "{IEEE} Transactions on Knowledge and Data Engineering"}
@STRING{IEEE_J_LT         = "{IEEE} Transactions on Learning Technologies"}
@STRING{IEEE_J_MAG        = "{IEEE} Transactions on Magnetics"}
@STRING{IEEE_J_MC         = "{IEEE} Transactions on Mobile Computing"}
@STRING{IEEE_J_ME         = "{IEEE} Transactions on Medical Electronics"}



% mechanical
@STRING{IEEE_J_MECH       = "{IEEE/ASME} Transactions on Mechatronics"}
@STRING{IEEE_J_MEMS       = "{IEEE/ASME} Journal of Microelectromechanical Systems"}
@STRING{IEEE_J_MFT        = "{IEEE} Transactions on Manufacturing Technology"}
@STRING{IEEE_J_MGWL       = "{IEEE} Microwave and Guided Wave Letters"}
@STRING{IEEE_J_MI         = "{IEEE} Transactions on Medical Imaging"}
@STRING{IEEE_J_MIL        = "{IEEE} Transactions on Military Electronics"}
@STRING{IEEE_J_MM         = "{IEEE} Transactions on Multimedia"}
@STRING{IEEE_J_MMS        = "{IEEE} Transactions on Man-Machine Systems"}
@STRING{IEEE_J_MTT        = "{IEEE} Transactions on Microwave Theory and Techniques"}
@STRING{IEEE_J_MWCL       = "{IEEE} Microwave and Wireless Components Letters"}
@STRING{IEEE_J_NANO       = "{IEEE} Transactions on Nanotechnology"}
@STRING{IEEE_J_NB         = "{IEEE} Transactions on NanoBioscience"}
@STRING{IEEE_J_NET        = "{IEEE/ACM} Transactions on Networking"}
@STRING{IEEE_J_NN         = "{IEEE} Transactions on Neural Networks"}
@STRING{IEEE_J_NS         = "{IEEE} Transactions on Nuclear Science"}
@STRING{IEEE_J_NSM        = "{IEEE} Transactions on Network and Service Management"}
@STRING{IEEE_J_NSRE       = "{IEEE} Transactions on Neural Systems and Rehabilitation Engineering"}
@STRING{IEEE_J_OE         = "{IEEE} Journal of Oceanic Engineering"}
@STRING{IEEE_J_PAMI       = "{IEEE} Transactions on Pattern Analysis and Machine Intelligence"}
@STRING{IEEE_J_PC         = "{IEEE} Transactions on Professional Communication"}
@STRING{IEEE_J_PDS        = "{IEEE} Transactions on Parallel and Distributed Systems"}
@STRING{IEEE_J_PEL        = "{IEEE} Power Electronics Letters"}
@STRING{IEEE_J_PHP        = "{IEEE} Transactions on Parts, Hybrids and Packaging"}
@STRING{IEEE_J_PMP        = "{IEEE} Transactions on Parts, Materials and Packaging"}
@STRING{IEEE_J_PROC       = "Proceedings of the {IEEE}"}
@STRING{IEEE_J_PS         = "{IEEE} Transactions on Plasma Science"}
@STRING{IEEE_J_PSE        = "{IEEE} Journal of Product Safety Engineering"}



% optics, lightwave and photonics
@STRING{IEEE_J_PTL        = "{IEEE} Photonics Technology Letters"}
@STRING{IEEE_J_PWRAS      = "{IEEE} Transactions on Power Apparatus and Systems"}
@STRING{IEEE_J_PWRD       = "{IEEE} Transactions on Power Delivery"}
@STRING{IEEE_J_PWRE       = "{IEEE} Transactions on Power Electronics"}
@STRING{IEEE_J_PWRS       = "{IEEE} Transactions on Power Systems"}
@STRING{IEEE_J_R          = "{IEEE} Transactions on Reliability"}
% in 1989 JRA became RA
% in August 2004, RA split into ASE and RO
@STRING{IEEE_J_RA         = "{IEEE} Transactions on Robotics and Automation"}
@STRING{IEEE_J_RBME       = "{IEEE} Reviews in Biomedical Engineering"}
@STRING{IEEE_J_RE         = "{IEEE} Transactions on Rehabilitation Engineering"}
@STRING{IEEE_J_RFI        = "{IEEE} Transactions on Radio Frequency Interference"}
@STRING{IEEE_J_RO         = "{IEEE} Transactions on Robotics"}
@STRING{IEEE_J_SAP        = "{IEEE} Transactions on Speech and Audio Processing"}
@STRING{IEEE_J_SC         = "{IEEE} Transactions on Services Computing"}
@STRING{IEEE_J_SE         = "{IEEE} Transactions on Software Engineering"}



% sensors
@STRING{IEEE_J_SENSOR     = "{IEEE} Sensors Journal"}
@STRING{IEEE_J_SM         = "{IEEE} Transactions on Semiconductor Manufacturing"}
@STRING{IEEE_J_SMC        = "{IEEE} Transactions on Systems, Man, and Cybernetics"}
@STRING{IEEE_J_SMCA       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {A}: Systems and Humans"}
@STRING{IEEE_J_SMCB       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {B}: Cybernetics"}
@STRING{IEEE_J_SMCC       = "{IEEE} Transactions on Systems, Man, and Cybernetics---Part {C}: Applications and Reviews"}
@STRING{IEEE_J_SP         = "{IEEE} Transactions on Signal Processing"}



% circuits, signals, systems, audio and controls
@STRING{IEEE_J_SPL        = "{IEEE} Signal Processing Letters"}
@STRING{IEEE_J_SSC        = "{IEEE} Transactions on Systems Science and Cybernetics"}
@STRING{IEEE_J_STARS      = "{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing"}
@STRING{IEEE_J_STSP       = "{IEEE} Journal of Selected Topics in Signal Processing"}
@STRING{IEEE_J_SU         = "{IEEE} Transactions on Sonics and Ultrasonics"}
@STRING{IEEE_J_SYST       = "{IEEE} Systems Journal"}



% CAD
@STRING{IEEE_J_TCAD       = "{IEEE} Journal on Technology in Computer Aided Design"}
@STRING{IEEE_J_TJMJ       = "{IEEE} Translation Journal on Magnetics in Japan"}
@STRING{IEEE_J_UE         = "{IEEE} Transactions on Ultrasonics Engineering"}
@STRING{IEEE_J_UFFC       = "{IEEE} Transactions on Ultrasonics, Ferroelectrics, and Frequency Control"}
@STRING{IEEE_J_VC         = "{IEEE} Transactions on Vehicular Communications"}
@STRING{IEEE_J_VCG        = "{IEEE} Transactions on Visualization and Computer Graphics"}



% VLSI
@STRING{IEEE_J_VLSI       = "{IEEE} Transactions on Very Large Scale Integration ({VLSI}) Systems"}
@STRING{IEEE_J_VT         = "{IEEE} Transactions on Vehicular Technology"}
@STRING{IEEE_J_WCOM       = "{IEEE} Transactions on Wireless Communications"}






% IEEE Magazines



@STRING{IEEE_M_AES        = "{IEEE} Aerospace and Electronics Systems Magazine"}
@STRING{IEEE_M_AP         = "{IEEE} Antennas and Propagation Magazine"}
@STRING{IEEE_M_ASSP       = "{IEEE} {ASSP} Magazine"}
@STRING{IEEE_M_C          = "{IEEE} Computer"}
@STRING{IEEE_M_CAP        = "{IEEE} Computer Applications in Power"}
@STRING{IEEE_M_CAS        = "{IEEE} Circuits and Systems Magazine"}
@STRING{IEEE_M_CD         = "{IEEE} Circuits and Devices Magazine"}
@STRING{IEEE_M_CGA        = "{IEEE} Computer Graphics and Applications"}
@STRING{IEEE_M_CIM        = "{IEEE} Computational Intelligence Magazine"}
@STRING{IEEE_M_COM        = "{IEEE} Communications Magazine"}
@STRING{IEEE_M_COMSOC     = "{IEEE} Communications Society Magazine"}
@STRING{IEEE_M_CONC       = "{IEEE} Concurrency"}
@STRING{IEEE_M_CS         = "{IEEE} Control Systems Magazine"}
% CSEM changed to CSE in 1999
@STRING{IEEE_M_CSE        = "{IEEE} Computing in Science and Engineering"}
@STRING{IEEE_M_CSEM       = "{IEEE} Computational Science and Engineering Magazine"}
@STRING{IEEE_M_DTC        = "{IEEE} Design and Test of Computers"}
@STRING{IEEE_M_EI         = "{IEEE} Electrical Insulation Magazine"}
@STRING{IEEE_M_EMB        = "{IEEE} Engineering in Medicine and Biology Magazine"}
@STRING{IEEE_M_EMR        = "{IEEE} Engineering Management Review"}
@STRING{IEEE_M_ETR        = "{IEEE} ElectroTechnology Review"}
@STRING{IEEE_M_EXP        = "{IEEE} Expert"}
@STRING{IEEE_M_HIST       = "{IEEE} Annals of the History of Computing"}
@STRING{IEEE_M_IA         = "{IEEE} Industry Applications Magazine"}
@STRING{IEEE_M_IC         = "{IEEE} Internet Computing"}
@STRING{IEEE_M_IE         = "{IEEE} Industrial Electronics Magazine"}
@STRING{IEEE_M_IM         = "{IEEE} Instrumentation and Measurement Magazine"}
@STRING{IEEE_M_IS         = "{IEEE} Intelligent Systems"}
@STRING{IEEE_M_ITP        = "{IEEE} {IT} Professional"}
@STRING{IEEE_M_ITS        = "{IEEE} Intelligent Transportation Systems Magazine"}
@STRING{IEEE_M_MICRO      = "{IEEE} Micro"}
@STRING{IEEE_M_MM         = "{IEEE} Multimedia"}
@STRING{IEEE_M_MW         = "{IEEE} Microwave Magazine"}
@STRING{IEEE_M_NANO       = "{IEEE} Nanotechnology Magazine"}
@STRING{IEEE_M_NET        = "{IEEE} Network"}
@STRING{IEEE_M_PCOM       = "{IEEE} Personal Communications Magazine"}
% CAP and PER merged to form PE in 2003
@STRING{IEEE_M_PE         = "{IEEE} Power and Energy Magazine"}
@STRING{IEEE_M_PER        = "{IEEE} Power Engineering Review"}
@STRING{IEEE_M_POT        = "{IEEE} Potentials"}
@STRING{IEEE_M_PVC        = "{IEEE} Pervasive Computing"}
@STRING{IEEE_M_RA         = "{IEEE} Robotics and Automation Magazine"}
@STRING{IEEE_M_S          = "{IEEE} Software"}
@STRING{IEEE_M_SAP        = "{IEEE} Security and Privacy"}
@STRING{IEEE_M_SP         = "{IEEE} Signal Processing Magazine"}
@STRING{IEEE_M_SPECT      = "{IEEE} Spectrum"}
@STRING{IEEE_M_TODAY      = "Today's Engineer"}
@STRING{IEEE_M_TS         = "{IEEE} Technology and Society Magazine"}
@STRING{IEEE_M_VT         = "{IEEE} Vehicular Technology Magazine"}
@STRING{IEEE_M_WC         = "{IEEE} Wireless Communications Magazine"}






% IEEE Online Publications



@STRING{IEEE_O_CSTO        = "{IEEE} Communications Surveys and Tutorials"}
@STRING{IEEE_O_DSO         = "{IEEE} Distributed Systems Online"}


@STRING{J-AGU = {{EOS}, Transactions of the American Geophysical Union}}
@STRING{J-AI = {Artificial Intelligence}}
@STRING{J-AJA = {American Journal of Archaeology}}
@STRING{J-AMAI = {Annals of Mathematics and Artifical Intelligence}}
@STRING{J-AMS = {The Annals of Mathematical Statistics}}
@STRING{J-AR = {Autonomous Robots}}
@STRING{J-ASME-JBE = {Transactions of the ASME---Journal of Basic Engineering}}
@STRING{J-CSR = {Continental Shelf Research}}
@STRING{J-CVIU = {Computer Vision and Image Understanding}}
@STRING{J-DSR1 = {Deep Sea Research I}}
@STRING{J-EPSL = {Earth and Planetary Science Letters}}
@STRING{J-HI = {Hydro International}}
@STRING{J-IEEE = {Proceedings of the {IEEE}}}
@STRING{J-IJCV = {International Journal of Computer Vision}}
@STRING{J-IJME = {International Journal of Maritime Engineering}}
@STRING{J-IJNA = {International Journal of Nautical Archaeology}}
@STRING{J-IJOPE = {International Journal of Offshore and Polar Engineering}}
@STRING{J-IJPRAI = {International Journal of Pattern Recognition and Artifical Intelligence}}
@STRING{J-IJRR = {International Journal of Robotics Research}}
@STRING{J-IJSR = {International Journal of Systems Science}}
@STRING{J-IVC = {Image and Vision Computing}}
@STRING{J-JAIR = {Journal of Artificial Intelligence Research}}
@STRING{J-JAOT = {Journal of Atmospheric and Oceanic Technology}}
@STRING{J-JAS = {Journal of Applied Statistics}}
@STRING{J-JASA = {Journal of the American Statistical Association}}
@STRING{J-JASA2 = {Journal of the Acoustical Society of America}}
@STRING{J-JEME = {Journal of Engineering for the Maritime Environment}}
@STRING{J-JFA = {Journal of Field Archaeology}}
@STRING{J-JFR = {Journal of Field Robotics}}
@STRING{J-JGLR = {Journal of Great Lakes Research}}
@STRING{J-JGR = {Journal of Geophysical Research}}
@STRING{J-JIN = {Journal of the Institute of Navigation}}
@STRING{J-JIS = {Journal of Infrastructure Systems}}
@STRING{J-JMIV = {Journal of Mathematical Imaging and Vision}}
@STRING{J-JMLR = {Journal of Machine Learning Research}}
@STRING{J-JMS = {{ICES} Journal of Marine Science}}
@STRING{J-JMST = {Journal of Marine Science and Technology}}
@STRING{J-JOSA = {Journal of the Optical Society of America}}
@STRING{J-JOT = {Journal of Ocean Technology}}
@STRING{J-JPT = {Journal of Petroleum Technology}}
@STRING{J-JR = {Journal of Robotics}}
@STRING{J-LNCS = {Lecture Notes in Computer Science, Springer-Verlag}}
@STRING{J-MC = {Mathematics of Computation}}
@STRING{J-MEPS = {Marine Ecology Progress Series}}
@STRING{J-ML = {Machine Learning}}
@STRING{J-MTS = {Marine Technology Society Journal}}
@STRING{J-MVA = {Machine Vision and Applications}}
@STRING{J-NEJ = {Naval Engineers Journal}}
@STRING{J-NRR = {Naval Research Reviews}}
@STRING{J-OE = {Optical Engineering}}
@STRING{J-PNAS = {Proceedings of the National Academy of Sciences of the United States of America}}
@STRING{J-RAL = {{IEEE} Robotics and Automation Letters}}
@STRING{J-RAS = {Robotics and Autonomous Systems}}
@STRING{J-SP = {Signal Processing}}
@STRING{J-SSTA = {Subsurface Sensing Technologies and Applications}}
@STRING{J-ST = {Sea Technology}}
@STRING{NOTE-APPEAR = {{{Accepted, To Appear}}}}
@STRING{NOTE-CONDITIONAL = {{{Conditionally Accepted}}}}
@STRING{NOTE-PRESS = {{{In Press}}}}
@STRING{NOTE-SUBMITTED = {{{Submitted, Under Review}}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% journals

@Article{Hartley2020Article,
  author   = {Ross Hartley and Maani Ghaffari and Ryan M. Eustice and Jessy W. Grizzle},
  journal  = J-IJRR,
  title    = {Contact-Aided Invariant Extended {Kalman} Filtering for Robot State Estimation},
  year     = {2020},
  number   = {4},
  pages    = {402--430},
  volume   = {39},
  abstract = {Legged robots require knowledge of pose and velocity in order to
	maintain stability and execute walking paths. Current solutions either
	rely on vision data, which is susceptible to environmental and lighting
	conditions, or fusion of kinematic and contact data with measurements from
	an inertial measurement unit (IMU). In this work, we develop a contact-aided
	invariant extended Kalman filter (InEKF) using the theory of Lie groups
	and invariant observer design. This filter combines contact-inertial dynamics
	with forward kinematic corrections to estimate pose and velocity along with
	all current contact points. We show that the error dynamics follows a log-linear
	autonomous differential equation with several important consequences: (a) the
	observable state variables can be rendered convergent with a domain of attraction that
	is independent of the system’s trajectory; (b) unlike the standard EKF, neither
	the linearized error dynamics nor the linearized observation model depend on the
	current state estimate, which (c) leads to improved convergence properties
	and (d) a local observability matrix that is consistent with the underlying nonlinear
	system. Furthermore, we demonstrate how to include IMU biases, add/remove contacts,
	and formulate both world-centric and robo-centric versions. We compare the convergence
	of the proposed InEKF with the commonly used quaternion-based extended Kalman filter
	(EKF) through both simulations and experiments on a Cassie-series bipedal robot.
	Filter accuracy is analyzed using motion capture, while a LiDAR mapping experiment
	provides a practical use case. Overall, the developed contact-aided InEKF provides
	better performance in comparison with the quaternion-based EKF as a result of exploiting
	symmetries present in system.},
  doi      = {10.1177/0278364919894385},
  groups   = {CURLY},
  url      = {https://arxiv.org/pdf/1904.09251.pdf},
}

@Article{GhaffariJadidi2019Article,
  author    = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Dissanayake, Gamini},
  journal   = J-IJRR,
  title     = {Sampling-based incremental information gathering with applications to robotic exploration and environmental monitoring},
  year      = {2019},
  number    = {6},
  pages     = {658--685},
  volume    = {38},
  abstract  = {We propose a sampling-based motion-planning algorithm equipped with an information-theoretic convergence criterion for incremental informative motion planning. The proposed approach allows dense map representations and incorporates the full state uncertainty into the planning process. The problem is formulated as a constrained maximization problem. Our approach is built on rapidly exploring information-gathering algorithms and benefits from the advantages of sampling-based optimal motion-planning algorithms. We propose two information functions and their variants for fast and online computations. We prove an information-theoretic convergence for an entire exploration and information-gathering mission based on the least upper bound of the average map entropy. A natural automatic stopping criterion for information-driven motion control results from the convergence analysis. We demonstrate the performance of the proposed algorithms using three scenarios: comparison of the proposed information functions and sensor configuration selection, robotic exploration in unknown environments, and a wireless signal strength monitoring task in a lake from a publicly available dataset collected using an autonomous surface vehicle.},
  doi       = {https://doi.org/10.1177%2F0278364919844575},
  publisher = {SAGE Publications},
  url       = {https://arxiv.org/pdf/1607.01883.pdf},
}

@Article{Gan2020Article,
  author   = {Lu Gan and Ray Zhang and Jessy W. Grizzle and Ryan M. Eustice and Maani Ghaffari},
  journal  = J-RAL,
  title    = {{Bayesian} Spatial Kernel Smoothing for Scalable Dense Semantic Mapping},
  year     = {2020},
  number   = {2},
  pages    = {790-797},
  volume   = {5},
  abstract = {This article develops a Bayesian continuous 3D semantic occupancy map from noisy 
	point clouds by generalizing the Bayesian kernel inference model for building occupancy 
	maps, a binary problem, to semantic maps, a multi-class problem. The proposed method 
	provides a unified probabilistic model for both occupancy and semantic probabilities 
	and nicely reverts to the original occupancy mapping framework when only one occupied
	class exists in obtained measurements. The Bayesian spatial kernel inference relaxes
	the independent grid assumption and brings smoothness and continuity to the map inference,
	enabling to exploit local correlations present in the environment and increasing the performance.
	The accompanying software uses multi-threading and vectorization, and runs at about 2 Hz on
	a laptop CPU. Evaluations using multiple sequences of stereo camera and LiDAR datasets show
	that the proposed method consistently outperforms current baselines. We also present a
	qualitative evaluation using data collected with a bipedal robot platform on the University
	of Michigan - North Campus.},
  doi      = {10.1109/LRA.2020.2965390},
  groups   = {CURLY},
  url      = {https://arxiv.org/pdf/1909.04631.pdf},
}

@Article{Parkison2019Article,
  author   = {Steven A. Parkison and Maani Ghaffari and Lu Gan and Ray Zhang and Arash K. Ushani and Ryan M. Eustice},
  journal  = J-RAL,
  title    = {Boosting Shape Registration Algorithms via Reproducing Kernel {Hilbert} Space Regularizers},
  year     = {2019},
  number   = {4},
  pages    = {4563--4570},
  volume   = {4},
  abstract = {The essence of most shape registration algorithms is to find correspondences
	between two point clouds and then to solve for a rigid body transformation that
	aligns the geometry. The main drawback is that the point clouds are obtained by
	placing the sensor at different views; consequently, the two matched points most
	likely do not correspond to the same physical point in the real environment. In other
	words, the point cloud is a discrete representation of the shape geometry. Alternatively,
	a point cloud measurement can be seen as samples from geometry, and a function can be learned
	for a continuous representation using regression techniques such as kernel methods. 
	To boost registration algorithms, this work develops a novel class of regularizers modeled
	in the Reproducing Kernel Hilbert Space (RKHS) that ensures correspondences are also 
	consistent in an abstract vector space of functions such as intensity surface.
	Furthermore, the proposed RKHS regularizer is agnostic to the choice of the registration
	cost function which is desirable. The evaluations on experimental data confirm the 
	effectiveness of the proposed regularizer using RGB-D and LIDAR sensors.},
  doi      = {10.1109/LRA.2019.2932865},
  groups   = {CURLY},
  url      = {https://aushani.com/pdfs/sparkison-2019a.pdf},
}

@Article{Hashemi2011Article,
  author    = {Hashemi, Ehsan and Ghaffari Jadidi, Maani and Ghaffari Jadidi, Navid},
  journal   = J-RAS,
  title     = {Model-based PI--fuzzy control of four-wheeled omni-directional mobile robots},
  year      = {2011},
  number    = {11},
  pages     = {930--942},
  volume    = {59},
  abstract  = {The purpose of this study is to suggest and examine a PI–fuzzy path planner and associated low-level control system for a linear discrete dynamic model of omni-directional mobile robots to obtain optimal inputs for drivers. Velocity and acceleration filtering is also implemented in the path planner to satisfy planning prerequisites and prevent slippage. Regulated drivers’ rotational velocities and torques greatly affect the ability of these robots to perform trajectory planner tasks. These regulated values are examined in this research by setting up an optimal controller. Introducing optimal controllers such as linear quadratic tracking for multi-input–multi-output control systems in acceleration and deceleration is one of the essential subjects for motion control of omni-directional mobile robots. The main topics presented and discussed in this article are improvements in the presented discrete-time linear quadratic tracking approach such as the low-level controller and combined PI–fuzzy path planner with appropriate speed monitoring algorithm such as the high-level one in conditions both with and without external disturbance. The low-level tracking controller presented in this article provides an optimal solution to minimize the differences between the reference trajectory and the system output. The efficiency of this approach is also compared with that of previous PID controllers which employ kinematic modeling. Utilizing the new approach in trajectory-planning controller design results in more precise and appropriate outputs for the motion of four-wheeled omni-directional mobile robots, and the modeling and experimental results confirm this issue.},
  doi       = {https://doi.org/10.1016/j.robot.2011.07.002},
  publisher = {North-Holland},
}

@Article{Valiente2015Article,
  author    = {Valiente, David and Ghaffari Jadidi, Maani and Valls Miro, Jaime and Gil, Arturo and Reinoso, Oscar},
  journal   = J-RAS,
  title     = {Information-based view initialization in visual SLAM with a single omnidirectional camera},
  year      = {2015},
  pages     = {93--104},
  volume    = {72},
  abstract  = {This paper presents a novel mechanism to initiate new views within the map building process for an EKF-based visual SLAM (Simultaneous Localization and Mapping) approach using omnidirectional images. In presence of non-linearities, the EKF is very likely to compromise the final estimation. Particularly, the omnidirectional observation model induces non-linear errors, thus it becomes a potential source of uncertainty. To deal with this issue we propose a novel mechanism for view initialization which accounts for information gain and losses more efficiently. The main outcome of this contribution is the reduction of the map uncertainty and thus the higher consistency of the final estimation. Its basis relies on a Gaussian Process to infer an information distribution model from sensor data. This model represents feature points existence probabilities and their information content analysis leads to the proposed view initialization scheme. To demonstrate the suitability and effectiveness of the approach we present a series of real data experiments conducted with a robot equipped with a camera sensor and map model solely based on omnidirectional views. The results reveal a beneficial reduction on the uncertainty but also on the error in the pose and the map estimate.},
  doi       = {https://doi.org/10.1016/j.robot.2015.05.005},
  publisher = {North-Holland},
}

@Article{GhaffariJadidi2018Article,
  author    = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Dissanayake, Gamini},
  journal   = J-AR,
  title     = {Gaussian processes autonomous mapping and exploration for range-sensing mobile robots},
  year      = {2018},
  number    = {2},
  pages     = {273--290},
  volume    = {42},
  abstract  = {Most of the existing robotic exploration schemes use occupancy grid representations and geometric targets known as frontiers. The occupancy grid representation relies on the assumption of independence between grid cells and ignores structural correlations present in the environment. We develop a Gaussian processes (GPs) occupancy mapping technique that is computationally tractable for online map building due to its incremental formulation and provides a continuous model of uncertainty over the map spatial coordinates. The standard way to represent geometric frontiers extracted from occupancy maps is to assign binary values to each grid cell. We extend this notion to novel probabilistic frontier maps computed efficiently using the gradient of the GP occupancy map. We also propose a mutual information-based greedy exploration technique built on that representation that takes into account all possible future observations. A major advantage of high-dimensional map inference is the fact that such techniques require fewer observations, leading to a faster map entropy reduction during exploration for map building scenarios. Evaluations using the publicly available datasets show the effectiveness of the proposed framework for robotic mapping and exploration tasks.},
  doi       = {10.1007/s10514-017-9668-3},
  publisher = {Springer US},
  url       = {https://arxiv.org/pdf/1605.00335.pdf},
}

@Article{GhaffariJadidi2017Article,
  author    = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Dissanayake, Gamini},
  journal   = J-RAL,
  title     = {Warped Gaussian Processes Occupancy Mapping With Uncertain Inputs},
  year      = {2017},
  number    = {2},
  pages     = {680--687},
  volume    = {2},
  abstract  = {In this paper, we study extensions to the Gaussian processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected submap notions to deal with uncertain inputs. In the second problem, we account for the complication of the robot's perception noise using warped Gaussian processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs improve the map quality.},
  doi       = {10.1109/LRA.2017.2651154},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/1701.00925},
}

@Article{Mangelson2020Article,
  author    = {Joshua G. Mangelson and Maani Ghaffari and Ram Vasudevan and Ryan M. Eustice},
  journal   = {IEEE Transactions on Robotics},
  title     = {Characterizing the Uncertainty of Jointly Distributed Poses in the Lie Algebra},
  year      = {2020},
  issn      = {1941-0468},
  number    = {5},
  pages     = {1371--1388},
  volume    = {36},
  abstract  = {An accurate characterization of pose uncertainty is essential for safe autonomous navigation. Early pose uncertainty characterization methods proposed by Smith, Self, and Cheeseman (SCC) used coordinate-based first-order methods to propagate uncertainty through nonlinear functions such as pose composition (head-to-tail), pose inversion, and relative pose extraction (tail-to-tail). Characterizing uncertainty in the Lie algebra of the special Euclidean group results in better uncertainty estimates. However, existing Lie-group-based uncertainty propagation techniques assume that individual poses are independent. After solving a pose graph, however, the entire trajectory is jointly distributed as factors induce correlation. Hence, the independence assumption does not capture reality. In addition, prior work has focused primarily on the pose composition operation. This article develops a framework for modeling the uncertainty of jointly distributed poses and describes how to perform the equivalent of the SSC pose operations while characterizing uncertainty in the Lie algebra. Evaluation on simulated and open-source datasets shows that the proposed methods result in more accurate uncertainty estimates and thus more accurate filtering of potential loop closures. An accompanying C++ library implementation is also released.},
  date      = {Oct. 2020},
  doi       = {10.1109/TRO.2020.2994457},
  file      = {:- Characterizing the Uncertainty of Jointly Distributed Poses in the Lie Algebra.html:URL},
  issue     = {5},
  keywords  = {Uncertainty, Algebra, Simultaneous localization and mapping, Robot kinematics, Maximum likelihood estimation, Correlation, Lie algebra, Lie group, matrix groups, mobile robotics, rigid body transformation, simultaneous localization and mapping (SLAM), state estimation, uncertainty propagation},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/1906.07795.pdf},
}

@Article{Huang2021Article,
  author    = {Jiunn-Kai Huang and Shoutian Wang and Maani Ghaffari and Jessy W. Grizzle},
  journal   = {IEEE Robotics and Automation Letters},
  title     = {LiDARTag: A Real-Time Fiducial Tag System for Point Clouds},
  year      = {2021},
  issn      = {2377-3774},
  pages     = {4875--4882},
  volume    = {6},
  abstract  = {Image-based fiducial markers are useful in problems such as object tracking in cluttered or textureless environments, camera (and multi-sensor) calibration tasks, and vision-based simultaneous localization and mapping (SLAM). The state-of-the-art fiducial marker detection algorithms rely on the consistency of the ambient lighting. This article introduces LiDARTag, a novel fiducial tag design and detection algorithm suitable for light detection and ranging (LiDAR) point clouds. The proposed method runs in real-time and can process data at 100 Hz, which is faster than the currently available LiDAR sensor frequencies. Because of the LiDAR sensors' nature, rapidly changing ambient lighting will not affect the detection of a LiDARTag; hence, the proposed fiducial marker can operate in a completely dark environment. In addition, the LiDARTag nicely complements and is compatible with existing visual fiducial markers, such as AprilTags, allowing for efficient multi-sensor fusion and calibration tasks. We further propose a concept of minimizing a fitting error between a point cloud and the marker's template to estimate the marker's pose. The proposed method achieves millimeter error in translation and a few degrees in rotation. Due to LiDAR returns' sparsity, the point cloud is lifted to a continuous function in a reproducing kernel Hilbert space where the inner product can be used to determine a marker's ID. The experimental results, verified by a motion capture system, confirm that the proposed method can reliably provide a tag's pose and unique ID code. The rejection of false positives is validated on the Google Cartographer indoor dataset and the Honda H3D outdoor dataset. All implementations are coded in C++ and are available at https://github.com/UMich-BipedLab/LiDARTag.},
  date      = {July 2021},
  doi       = {10.1109/LRA.2021.3070302},
  file      = {:- LiDARTag_ a Real Time Fiducial Tag System for Point Clouds.html:URL},
  issue     = {3},
  keywords  = {Laser radar, Three-dimensional displays, Cameras, Payloads, Lighting, Detectors, Visualization, Range sensing, visual tracking, object detection, segmentation and categorization, computer vision for automation},
  publisher = {IEEE},
}

@Article{Song2022Article,
  author    = {Jingwei Song and Mitesh Patel and Maani Ghaffari},
  journal   = {IEEE Robotics and Automation Letters},
  title     = {Fusing Convolutional Neural Network and Geometric Constraint for Image-Based Indoor Localization},
  year      = {2022},
  issn      = {2377-3774},
  number    = {2},
  pages     = {1674--1681},
  volume    = {7},
  abstract  = {This letter proposes a new image-based localization framework that explicitly localizes the camera/robot by fusing Convolutional Neural Network (CNN) and sequential images&amp;#x2019; geometric constraints. The camera is localized using a single or few observed images and training images with 6-degree-of-freedom pose labels. A Siamese network structure is adopted to train an image descriptor network, and the visually similar candidate image in the training set is retrieved to localize the testing image geometrically. Meanwhile, a probabilistic motion model predicts the pose based on a constant velocity assumption. The two estimated poses are finally fused using their uncertainties to yield an accurate pose prediction. This method leverages the geometric uncertainty and is applicable in indoor scenarios predominated by diffuse illumination. Experiments on simulation and real data sets demonstrate the efficiency of our proposed method. The results further show that combining the CNN-based framework with geometric constraint achieves better accuracy when compared with CNN-only methods, especially when the training data size is small.},
  date      = {April 2022},
  doi       = {10.1109/LRA.2022.3140832},
  file      = {:- Fusing Convolutional Neural Network and Geometric Constraint for Image Based Indoor Localization.html:URL},
  issue     = {2},
  keywords  = {Training, Uncertainty, Location awareness, Testing, Cameras, Simultaneous localization and mapping, Predictive models, Vision-based navigation, localization, probability and statistical methods},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2201.01408.pdf},
}

@Article{Song2022Articlea,
  author    = {Jingwei Song and Mitesh Patel and Ashkan Jasour and Maani Ghaffari},
  journal   = {IEEE Robotics and Automation Letters},
  title     = {A Closed-Form Uncertainty Propagation in Non-Rigid Structure From Motion},
  year      = {2022},
  issn      = {2377-3774},
  pages     = {6479--6486},
  volume    = {7},
  abstract  = {Semi-Definite Programming (SDP) with low-rank prior has been widely applied in Non-Rigid Structure from Motion (NRSfM). A low-rank constraint avoids the inherent ambiguity of the basis number selection in conventional base-shape or base-trajectory methods. Despite SDP-based NRSfM&amp;#x2019;s efficiency, it remains unclear how to propagate the noisy tracked feature points&amp;#x2019; uncertainty to the 3D recovered shape in SDP-based NRSfM formulation. This paper presents a closed-form statistical inference for the element-wise uncertainty propagation of the estimated deforming 3D shape points in the exact low-rank SDP-based NRSfM. Then, we extend the exact low-rank uncertainty propagation to the approximate low-rank scenario with an optimal numerical rank selection method. The proposed method provides an independent module to the SDP-based method and only requires the statistical information of the input 2D trackings. Extensive experiments show that the major uncertainty in the recovered 3D points follows normal distribution, the proposed method quantifies the uncertainty accurately, and it has desirable effects on the routinely SDP low-rank based NRSfM solver.},
  date      = {July 2022},
  doi       = {10.1109/LRA.2022.3173733},
  file      = {:Song2022a - A Closed Form Uncertainty Propagation in Non Rigid Structure from Motion.html:URL},
  issue     = {3},
  keywords  = {Uncertainty, Shape, Estimation, Three-dimensional displays, Cameras, Noise measurement, Simultaneous localization and mapping, NRSfM, SDP, uncertainty quantification},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2005.04810.pdf},
}

@Article{Gan2022Article,
  author   = {Gan, Lu and Kim, Youngji and Grizzle, Jessy W. and Walls, Jeffrey M. and Kim, Ayoung and Eustice, Ryan M. and Ghaffari, Maani},
  journal  = {IEEE Transactions on Robotics},
  title    = {Multitask Learning for Scalable and Dense Multilayer Bayesian Map Inference},
  year     = {2022},
  pages    = {1-19},
  abstract = {In this article, we present a novel and flexible multitask multilayer Bayesian mapping framework with readily extendable attribute layers. The proposed framework goes beyond modern metric-semantic maps to provide even richer environmental information for robots in a single mapping formalism while exploiting intralayer and interlayer correlations. It removes the need for a robot to access and process information from many separate maps when performing a complex task, advancing the way robots interact with their environments. To this end, we design a multitask deep neural network with attention mechanisms as our front-end to provide heterogeneous observations for multiple map layers simultaneously. Our back-end runs a scalable closed-form Bayesian inference with only logarithmic time complexity. We apply the framework to build a dense robotic map, including metric-semantic occupancy and traversability layers. Traversability ground truth labels are automatically generated from exteroceptive sensory data in a self-supervised manner. We present extensive experimental results on publicly available datasets and data collected by a three-dimensional bipedal robot platform and show reliable mapping performance in different environments. Finally, we also discuss how the current framework can be extended to incorporate more information, such as friction, signal strength, temperature, and physical quantity concentration using Gaussian map layers. The software for reproducing the presented results or running on customized data is made publicly available.},
  doi      = {10.1109/TRO.2022.3197106},
  url      = {https://arxiv.org/abs/2106.14986},
}

@Article{Fu2021Article,
  author   = {Fu, Bo and Kathuria, Tribhi and Rizzo, Denise and Castanier, Matthew and Yang, X Jessie and Ghaffari, Maani and Barton, Kira},
  journal  = {Journal of Autonomous Vehicles and Systems},
  title    = {Simultaneous Human-Robot Matching and Routing for Multi-Robot Tour Guiding Under Time Uncertainty},
  year     = {2021},
  issn     = {2690-702X},
  month    = feb,
  note     = {041005},
  number   = {4},
  volume   = {1},
  abstract = {This work presents a framework for multi-robot tour guidance in a partially known environment with uncertainty, such as a museum. In the proposed centralized multi-robot planner, a simultaneous matching and routing problem (SMRP) is formulated to match the humans with robot guides according to their selected places of interest (POIs) and generate the routes and schedules for the robots according to uncertain spatial and time estimation. A large neighborhood search algorithm is developed to efficiently find sub-optimal low-cost solutions for the SMRP. The scalability and optimality of the multi-robot planner are evaluated computationally under different numbers of humans, robots, and POIs. The largest case tested involves 50 robots, 250 humans, and 50 POIs. Then, a photo-realistic multi-robot simulation platform was developed based on Habitat-AI to verify the tour guiding performance in an uncertain indoor environment. Results demonstrate that the proposed centralized tour planner is scalable, makes a smooth tradeoff in the plans under different environmental constraints, and can lead to robust performance with inaccurate uncertainty estimations (within a certain margin).},
  doi      = {https://doi.org/10.1115/1.4053428},
}

@Article{Wilson2022Article,
  author    = {Wilson, Joey and Song, Jingyu and Fu, Yuewei and Zhang, Arthur and Capodieci, Andrew and Jayakumar, Paramsothy and Barton, Kira and Ghaffari, Maani},
  journal   = {IEEE Robotics and Automation Letters},
  title     = {MotionSC: Data Set and Network for Real-Time Semantic Mapping in Dynamic Environments},
  year      = {2022},
  number    = {3},
  pages     = {8439-8446},
  volume    = {7},
  abstract  = {This work addresses a gap in semantic scene completion (SSC) data by creating a novel outdoor data set with accurate and complete dynamic scenes. Our data set is formed from randomly sampled views of the world at each time step, which supervises generalizability to complete scenes without occlusions or traces. We create SSC baselines from state-of-the-art open source networks and construct a benchmark real-time dense local semantic mapping algorithm, MotionSC, by leveraging recent 3D deep learning architectures to enhance SSC with temporal information. Our network shows that the proposed data set can quantify and supervise accurate scene completion in the presence of dynamic objects, which can lead to the development of improved dynamic mapping algorithms.},
  doi       = {10.1109/LRA.2022.3188435},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2203.07060.pdf},
}

@Article{Gan2022Articlea,
  author    = {Gan, Lu and Grizzle, Jessy W and Eustice, Ryan M and Ghaffari, Maani},
  journal   = {IEEE Robotics and Automation Letters},
  title     = {Energy-Based Legged Robots Terrain Traversability Modeling via Deep Inverse Reinforcement Learning},
  year      = {2022},
  number    = {4},
  pages     = {8807--8814},
  volume    = {7},
  abstract  = {This work reports ondeveloping a deep inverse reinforcement learning method for legged robots terrain traversability modeling that incorporates both exteroceptive and proprioceptive sensory data. Existing works use robot-agnostic exteroceptive environmental features or handcrafted kinematic features; instead, we propose to also learn robot-specific inertial features from proprioceptive sensory data for reward approximation in a single deep neural network. Incorporating the inertial features can improve the model fidelity and provide a reward that depends on the robot’s state during deployment. We train the reward network using the Maximum Entropy Deep Inverse Reinforcement Learning (MEDIRL) algorithm and propose simultaneously minimizing a trajectory ranking loss to deal with the suboptimality of legged robot demonstrations. The demonstrated trajectories are ranked by locomotion energy consumption, in order to learn an energy-aware reward function and a more energy-efficient policy than demonstration. We evaluate our method using a dataset collected by an MIT Mini-Cheetah robot and a Mini-Cheetah simulator. The code is publicly available.},
  doi       = {10.1109/LRA.2022.3188100},
  publisher = {IEEE},
  url       = {https://arxiv.org/pdf/2207.03034.pdf},
}

@Article{Clark2021Article,
  author   = {William Clark and Maani Ghaffari and Anthony Bloch},
  journal  = {Journal of Machine Learning Research},
  title    = {Nonparametric Continuous Sensor Registration},
  year     = {2021},
  number   = {271},
  pages    = {1-50},
  volume   = {22},
  abstract = {This paper develops a new mathematical framework that enables nonparametric joint semantic and geometric representation of continuous functions using data. The joint embedding is modeled by representing the processes in a reproducing kernel Hilbert space. The functions can be defined on arbitrary smooth manifolds where the action of a Lie group aligns them. The continuous functions allow the registration to be independent of a specific signal resolution. The framework is fully analytical with a closed-form derivation of the Riemannian gradient and Hessian. We study a more specialized but widely used case where the Lie group acts on functions isometrically. We solve the problem by maximizing the inner product between two functions defined over data, while the continuous action of the rigid body motion Lie group is captured through the integration of the flow in the corresponding Lie algebra. Low-dimensional cases are derived with numerical examples to show the generality of the proposed framework. The high-dimensional derivation for the special Euclidean group acting on the Euclidean space showcases the point cloud registration and bird's-eye view map registration abilities. An implementation of this framework for RGB-D cameras outperforms the state-of-the-art robust visual odometry and performs well in texture and structure-scarce environments.},
  url      = {http://jmlr.org/papers/v22/20-1468.html},
}

@Article{Ghaffari2022Article,
  author   = {Ghaffari, Maani and Zhang, Ray and Zhu, Minghan and Lin, Chien Erh and Lin, Tzu-Yuan and Teng, Sangli and Li, Tingjun and Liu, Tianyi and Song, Jingwei},
  journal  = {Frontiers in Robotics and AI},
  title    = {Progress in symmetry preserving robot perception and control through geometry and learning},
  year     = {2022},
  issn     = {2296-9144},
  volume   = {9},
  abstract = {This article reports on recent progress in robot perception and control methods developed by taking the symmetry of the problem into account. Inspired by existing mathematical tools for studying the symmetry structures of geometric spaces, geometric sensor registration, state estimator, and control methods provide indispensable insights into the problem formulations and generalization of robotics algorithms to challenging unknown environments. When combined with computational methods for learning hard-to-measure quantities, symmetry-preserving methods unleash tremendous performance. The article supports this claim by showcasing experimental results of robot perception, state estimation, and control in real-world scenarios.},
  doi      = {10.3389/frobt.2022.969380},
  url      = {https://www.frontiersin.org/articles/10.3389/frobt.2022.969380},
}

@Article{Unnikrishnan2022Article,
  author    = {Unnikrishnan, Aishwarya and Wilson, Joey and Gan, Lu and Capodieci, Andrew and Jayakumar, Paramsothy and Barton, Kira and Ghaffari, Maani},
  journal   = {IEEE Access},
  title     = {Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference},
  year      = {2022},
  pages     = {1-17},
  abstract  = {This paper reports on a dynamic semantic mapping framework that incorporates 3D scene
flow measurements into a closed-form Bayesian inference model. Existence of dynamic objects in the
environment can cause artifacts and traces in current mapping algorithms, leading to an inconsistent map
posterior. We leverage state-of-the-art semantic segmentation and 3D flow estimation using deep learning
to provide measurements for map inference. We develop a Bayesian model that propagates the scene
with flow and infers a 3D continuous (i.e., can be queried at arbitrary resolution) semantic occupancy
map outperforming its static counterpart. Extensive experiments using publicly available data sets show
that the proposed framework improves over its predecessors and input measurements from deep neural
networks consistently.},
  doi       = {10.1109/ACCESS.2022.3205329},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9882042},
}

@Comment{jabref-meta: databaseType:bibtex;}
