<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2013, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
// - put BibTeX type in a data-attribute

// Search settings
var searchAbstract = true;	// search in abstract
var searchReview = true;	// search in review

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, review, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	RefTypeKeys = new Array();

	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			RefTypeKeys[j] = allRows[i].getAttribute('data-reftype');
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/review
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/review/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1 || RefTypeKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchReview && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'review') {
		rev.className.indexOf('noshow') == -1?rev.className = 'review noshow':rev.className = 'review show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/review/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'review nextshow': rev.className = 'review';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchReview=!searchReview;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchReview){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}	
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; width: 50em; margin: auto auto; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { border: 1px gray none; width: 100%; empty-cells: show; border-spacing: 0em 0.1em; margin: 1em 0em; }
th, td { border: none; padding: 0.5em; vertical-align: top; text-align: justify; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.review td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom-style: none; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>

<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include review</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<tbody>
<tr id="Ghaffari2019" data-reftype="InProceedings" class="entry">
	<td>Ghaffari M, Clark W, Bloch A, Eustice RM and Grizzle JW (2019), <i>"Continuous Direct Sparse Visual Odometry from RGB-D Images"</i>, In Proceedings of the Robotics: Science and Systems Conference. FreiburgimBreisgau, Germany, June, 2019. , pp. 1-9.
  <p class="infolinks">[<a href="javascript:toggleInfo('Ghaffari2019','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ghaffari2019','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1904.02266.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Ghaffari2019" class="abstract noshow">
	<td><b>Abstract</b>: This paper reports on a novel formulation and evaluation of visual <br>	odometry from RGB-D images. Assuming a static scene, the developed theoretical<br>	framework generalizes the widely used direct energy formulation (photometric <br>	error minimization) technique for obtaining a rigid body transformation that <br>	aligns two overlapping RGB-D images to a continuous formulation. The continuity<br>	is achieved through functional treatment of the problem and representing the <br>	process models over RGB-D images in a reproducing kernel Hilbert space; consequently,<br>	the registration is not limited to the specific image resolution and the framework<br>	is fully analytical with a closed-form derivation of the gradient. We solve the<br>	problem by maximizing the inner product between two functions defined over RGB-D<br>	images, while the continuous action of the rigid body motion Lie group is captured<br>	through the integration of the flow in the corresponding Lie algebra. Energy-based<br>	approaches have been extremely successful and the developed framework in this paper<br>	shares many of their desired properties such as the parallel structure on both CPUs<br>	and GPUs, sparsity, semi-dense tracking, avoiding explicit data association which is<br>	computationally expensive, and possible extensions to the simultaneous localization<br>	and mapping frameworks. The evaluations on experimental data and comparison with<br>	the equivalent energy-based formulation of the problem confirm the effectiveness<br>	of the proposed technique, especially, when the lack of structure and texture in<br>	the environment is evident.</td>
</tr>
<tr id="bib_Ghaffari2019" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Ghaffari2019,
  author = {Maani Ghaffari and William Clark and Anthony Bloch and Ryan M. Eustice and Jessey W. Grizzle},
  title = {Continuous Direct Sparse Visual Odometry from RGB-D Images},
  booktitle = {Proceedings of the Robotics: Science and Systems Conference},
  year = {2019},
  pages = {1--9},
  url = {https://arxiv.org/pdf/1904.02266.pdf}
}
</pre></td>
</tr>
<tr id="Dhanjal2019" data-reftype="InProceedings" class="entry">
	<td>Dhanjal SS, Ghaffari M and Eustice RM (2019), <i>"DeepLocNet: Deep Observation Classification and Ranging Bias Regression for Radio Positioning Systems"</i>, In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. Macau, China, November, 2019. , pp. 3802-3809.
  <p class="infolinks">[<a href="javascript:toggleInfo('Dhanjal2019','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Dhanjal2019','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/2002.00484.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Dhanjal2019" class="abstract noshow">
	<td><b>Abstract</b>: WiFi technology has been used pervasively in fine-grained indoor localization,<br>	gesture recognition, and adaptive communication. Achieving better performance in these <br>	tasks generally boils down to differentiating Line-Of-Sight (LOS) from Non-Line-Of-Sight<br>	(NLOS) signal propagation reliably which generally requires expensive/specialized <br>	hardware due to the complex nature of indoor environments. Hence, the development of<br>	low-cost accurate positioning systems that exploit available infrastructure is not<br>	entirely solved. In this paper, we develop a framework for indoor localization and<br>	tracking of ubiquitous mobile devices such as smartphones using on-board sensors.<br>	We present a novel deep LOS/NLOS classifier which uses the Received Signal Strength<br>	Indicator (RSSI), and can classify the input signal with an accuracy of 85%. The proposed<br>	algorithm can globally localize and track a smartphone (or robot) with a priori unknown<br>	location, and with a semi-accurate prior map (error within 0.8m) of the WiFi Access Points<br>	(AP). Through simultaneously solving for the trajectory and the map of access points, we <br>	recover a trajectory of the device and corrected locations for the access points.<br>	Experimental evaluations of the framework show that localization accuracy is increased by <br>	using the trained deep network; furthermore, the system becomes robust to any error in<br>	the map of APs.</td>
</tr>
<tr id="bib_Dhanjal2019" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Dhanjal2019,
  author = {Sahib Singh Dhanjal and Maani Ghaffari and Ryan M. Eustice},
  title = {DeepLocNet: Deep Observation Classification and Ranging Bias Regression for Radio Positioning Systems},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2019},
  pages = {3802--3809},
  url = {https://arxiv.org/pdf/2002.00484.pdf}
}
</pre></td>
</tr>
<tr id="Parkison2018" data-reftype="InProceedings" class="entry">
	<td>Parkison SA, Gan L, Jadidi MG and Eustice RM (2018), <i>"Semantic Iterative Closest Point through Expectation-Maximization"</i>, In Proceedings of the British Machine Vision Conference. Newcastle, UK, September, 2018. , pp. 1-17.
  <p class="infolinks">[<a href="javascript:toggleInfo('Parkison2018','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Parkison2018','bibtex')">BibTeX</a>] [<a href="http://141.212.194.179/publications/sparkison-2018a.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Parkison2018" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we develop a novel point cloud registration algorithm<br>	that directly incorporates pixelated semantic measurements into the<br>	estimation of the relative transformation between two point clouds.<br>	The algorithm uses an Iterative Closest Point (ICP)-like scheme and<br>	performs joint semantic and geometric inference using the Expectation-Maximization<br>	technique in which semantic labels and point associations between<br>	two point clouds are treated as latent random variables. The minimization<br>	of the expected cost on the three-dimensional special Euclidean group,<br>	i.e., SE(3), yields the rigid body transformation between two point<br>	clouds. The evaluation on publicly available RGBD benchmarks shows<br>	that, in comparison with both the standard Generalized ICP (GICP)<br>	available in the Point Cloud Library and GICP on SE(3), the registration<br>	error is reduced.</td>
</tr>
<tr id="bib_Parkison2018" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Parkison2018,
  author = {Steven A. Parkison and Lu Gan and Maani Ghaffari Jadidi and Ryan M. Eustice},
  title = {Semantic Iterative Closest Point through Expectation-Maximization},
  booktitle = {Proceedings of the British Machine Vision Conference},
  year = {2018},
  pages = {1--17},
  url = {http://141.212.194.179/publications/sparkison-2018a.pdf}
}
</pre></td>
</tr>
<tr id="Hartley2018b" data-reftype="InProceedings" class="entry">
	<td>Hartley R, Mangelson J, Gan L, Jadidi MG, Walls JM, Eustice RM and Grizzle JW (2018), <i>"Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors"</i>, In Proceedings of the IEEE International Conference on Robotics and Automation. Brisbane, Australia, May, 2018. , pp. 4422-4429.
  <p class="infolinks">[<a href="javascript:toggleInfo('Hartley2018b','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hartley2018b','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1712.05873.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Hartley2018b" class="abstract noshow">
	<td><b>Abstract</b>: State-of-the-art robotic perception systems have achieved sufficiently<br>	good performance using Inertial Measurement Units (IMUs), cameras,<br>	and nonlinear optimization techniques, that they are now being deployed<br>	as technologies. However, many of these methods rely significantly<br>	on vision and often fail when visual tracking is lost due to lighting<br>	or scarcity of features. This paper presents a state-estimation technique<br>	for legged robots that takes into account the robot's kinematic model<br>	as well as its contact with the environment. We introduce forward<br>	kinematic factors and preintegrated contact factors into a factor<br>	graph framework that can be incrementally solved in real-time. The<br>	forward kinematic factor relates the robot's base pose to a contact<br>	frame through noisy encoder measurements. The preintegrated contact<br>	factor provides odometry measurements of this contact frame while<br>	accounting for possible foot slippage. Together, the two developed<br>	factors constrain the graph optimization problem allowing the robot's<br>	trajectory to be estimated. The paper evaluates the method using<br>	simulated and real sensory IMU and kinematic data from experiments<br>	with a Cassie-series robot designed by Agility Robotics. These preliminary<br>	experiments show that using the proposed method in addition to IMU<br>	decreases drift and improves localization accuracy, suggesting that<br>	its use can enable successful recovery from a loss of visual tracking.</td>
</tr>
<tr id="bib_Hartley2018b" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Hartley2018b,
  author = {Ross Hartley and Josh Mangelson and Lu Gan and Maani Ghaffari Jadidi and Jeffery M. Walls and Ryan M. Eustice and Jessy W. Grizzle},
  title = {Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation},
  year = {2018},
  pages = {4422--4429},
  url = {https://arxiv.org/pdf/1712.05873.pdf}
}
</pre></td>
</tr>
<tr id="Hartley2018a" data-reftype="InProceedings" class="entry">
	<td>Hartley R, Jadidi MG, Grizzle JW and Eustice RM (2018), <i>"Contact-Aided Invariant Extended Kalman Filtering for Legged Robot State Estimation"</i>, In Proceedings of the Robotics: Science and Systems Conference. Pittsburgh, PA, USA, June, 2018. , pp. 1-9.
  <p class="infolinks">[<a href="javascript:toggleInfo('Hartley2018a','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hartley2018a','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1805.10410.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Hartley2018a" class="abstract noshow">
	<td><b>Abstract</b>: This paper derives a contact-aided inertial navigation observer for<br>	a 3D bipedal robot using the theory of invariant observer design.<br>	Aided inertial navigation is fundamentally a nonlinear observer design<br>	problem; thus, current solutions are based on approximations of the<br>	system dynamics, such as an Extended Kalman Filter (EKF), which uses<br>	a system's Jacobian linearization along the current best estimate<br>	of its trajectory. On the basis of the theory of invariant observer<br>	design by Barrau and Bonnabel, and in particular, the Invariant EKF<br>	(InEKF), we show that the error dynamics of the point contact-inertial<br>	system follows a log-linear autonomous differential equation; hence,<br>	the observable state variables can be rendered convergent with a<br>	domain of attraction that is independent of the system's trajectory.<br>	Due to the log-linear form of the error dynamics, it is not necessary<br>	to perform a nonlinear observability analysis to show that when using<br>	an Inertial Measurement Unit (IMU) and contact sensors, the absolute<br>	position of the robot and a rotation about the gravity vector (yaw)<br>	are unobservable. We further augment the state of the developed InEKF<br>	with IMU biases, as the online estimation of these parameters has<br>	a crucial impact on system performance. We evaluate theconvergence<br>	of the proposed system with the commonly used quaternion-based EKF<br>	observer using a Monte-Carlo simulation. In addition, our experimental<br>	evaluation using a Cassie-series bipedal robot shows that the contact-aided<br>	InEKF provides better performance in comparison with the quaternion-based<br>	EKF as a result of exploiting symmetries present in the system dynamics.</td>
</tr>
<tr id="bib_Hartley2018a" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Hartley2018a,
  author = {Ross Hartley and Maani Ghaffari Jadidi and Jessy W. Grizzle and Ryan M. Eustice},
  title = {Contact-Aided Invariant Extended Kalman Filtering for Legged Robot State Estimation},
  booktitle = {Proceedings of the Robotics: Science and Systems Conference},
  year = {2018},
  pages = {1--9},
  url = {https://arxiv.org/pdf/1805.10410.pdf}
}
</pre></td>
</tr>
<tr id="Hartley2018" data-reftype="InProceedings" class="entry">
	<td>Hartley R, Jadidi MG, Gan L, Huang J-K, Grizzle JW and Eustice RM (2018), <i>"Hybrid Contact Preintegration for Visual-Inertial-Contact State Estimation within Factor Graphs"</i>, In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. Madrid, Spain, October, 2018. , pp. 3783-3790.
  <p class="infolinks">[<a href="javascript:toggleInfo('Hartley2018','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hartley2018','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1803.07531.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Hartley2018" class="abstract noshow">
	<td><b>Abstract</b>: The factor graph framework is a convenient modeling technique for<br>	robotic state estimation where states are represented as nodes, and<br>	measurements are modeled as factors. When designing a sensor fusion<br>	framework for legged robots, one often has access to visual, inertial,<br>	joint encoder, and contact sensors. While visual-inertial odometry<br>	has been studied extensively in this framework, the addition of a<br>	preintegrated contact factor for legged robots has been only recently<br>	proposed. This allowed for integration of encoder and contact measurements<br>	into existing factor graphs, however, new nodes had to be added to<br>	the graph every time contact was made or broken. In this work, to<br>	cope with the problem of switching contact frames, we propose a hybrid<br>	contact preintegration theory that allows contact information to<br>	be integrated through an arbitrary number of contact switches. The<br>	proposed hybrid modeling approach reduces the number of required<br>	variables in the nonlinear optimization problem by only requiring<br>	new states to be added alongside camera or selected keyframes. This<br>	method is evaluated using real experimental data collected from a<br>	Cassie-series robot where the trajectory of the robot produced by<br>	a motion capture system is used as a proxy for ground truth. The<br>	evaluation shows that inclusion of the proposed preintegrated hybrid<br>	contact factor alongside visual-inertial navigation systems improves<br>	estimation accuracy as well as robustness to vision failure, while<br>	its generalization makes it more accessible for legged platforms.</td>
</tr>
<tr id="bib_Hartley2018" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Hartley2018,
  author = {Ross Hartley and Maani Ghaffari Jadidi and Lu Gan and Jiunn-Kai Huang and Jessy W. Grizzle and Ryan M. Eustice},
  title = {Hybrid Contact Preintegration for Visual-Inertial-Contact State Estimation within Factor Graphs},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2018},
  pages = {3783--3790},
  url = {https://arxiv.org/pdf/1803.07531.pdf}
}
</pre></td>
</tr>
<tr id="GhaffariJadidi2018" data-reftype="InProceedings" class="entry">
	<td>Ghaffari Jadidi M, Patel M, Miro JV, Dissanayake G, Biehl J and Girgensohn A (2018), <i>"A Radio-Inertial Localization and Tracking System with BLE Beacons Prior Maps"</i>, In Proceedings of the IEEE International Conference on Indoor Positioning and Indoor Navigation. , pp. 1-8.
  <p class="infolinks">[<a href="javascript:toggleInfo('GhaffariJadidi2018','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('GhaffariJadidi2018','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1706.05569.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_GhaffariJadidi2018" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we develop a system for the low-cost indoor localization and tracking problem using radio signal strength indicator, Inertial Measurement Unit (IMU), and magnetometer sensors. We develop a novel and simplified probabilistic IMU motion model as the proposal distribution of the sequential Monte-Carlo technique to track the robot trajectory. Our algorithm can globally localize and track a robot with a priori unknown location, given an informative prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the problem as an optimization problem that serves as the Backend of the algorithm mentioned above (Front-end). Thus, by simultaneously solving for the robot trajectory and the map of BLE beacons, we recover a continuous and smooth trajectory of the robot, corrected locations of the BLE beacons, and the time-varying IMU bias. The evaluations achieved using hardware show that through the proposed closed-loop system the localization performance can be improved; furthermore, the system becomes robust to the error in the map of beacons by feeding back the optimized map to the Front-end.</td>
</tr>
<tr id="bib_GhaffariJadidi2018" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{GhaffariJadidi2018,
  author = {Ghaffari Jadidi, Maani and Patel, Mitesh and Miro, Jaime Valls and Dissanayake, Gamini and Biehl, Jacob and Girgensohn, Andreas},
  title = {A Radio-Inertial Localization and Tracking System with BLE Beacons Prior Maps},
  booktitle = {Proceedings of the IEEE International Conference on Indoor Positioning and Indoor Navigation},
  year = {2018},
  pages = {1--8},
  url = {https://arxiv.org/pdf/1706.05569.pdf}
}
</pre></td>
</tr>
<tr id="Jadidi2017" data-reftype="InProceedings" class="entry">
	<td>Jadidi MG, Gan L, Parkison SA, Li J and Eustice RM (2017), <i>"Gaussian processes semantic map representation"</i>, In RSS Workshop on Spatial-Semantic Representations in Robotics. Cambridge, MA, USA, July, 2017. 
  <p class="infolinks">[<a href="javascript:toggleInfo('Jadidi2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Jadidi2017','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1707.01532" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Jadidi2017" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we develop a high-dimensional map building technique<br>	that incorporates raw pixelated semantic measurements into the map<br>	representation. The proposed technique uses Gaussian Processes (GPs)<br>	multi-class classification for map inference and is the natural extension<br>	of GP occupancy maps from binary to multi-class form. The technique<br>	exploits the continuous property of GPs and, as a result, the map<br>	can be inferred with any resolution. In addition, the proposed GP<br>	Semantic Map (GPSM) learns the structural and semantic correlation<br>	from measurements rather than resorting to assumptions, and can flexibly<br>	learn the spatial correlation as well as any additional non-spatial<br>	correlation between map points. We extend the OctoMap to Semantic<br>	OctoMap representation and compare with the GPSM mapping performance<br>	using NYU Depth V2 dataset. Evaluations of the proposed technique<br>	on multiple partially labeled RGBD scans and labels from noisy image<br>	segmentation show that the GP semantic map can handle sparse measurements,<br>	missing labels in the point cloud, as well as noise corrupted labels.</td>
</tr>
<tr id="bib_Jadidi2017" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Jadidi2017,
  author = {Maani Ghaffari Jadidi and Lu Gan and Steven A. Parkison and Jie Li and Ryan M. Eustice},
  title = {Gaussian processes semantic map representation},
  booktitle = {RSS Workshop on Spatial-Semantic Representations in Robotics},
  year = {2017},
  url = {https://arxiv.org/pdf/1707.01532}
}
</pre></td>
</tr>
<tr id="GhaffariJadidi2017a" data-reftype="InProceedings" class="entry">
	<td>Ghaffari Jadidi M, Patel M and Valls Miro J (2017), <i>"Gaussian processes online observation classification for RSSI-based low-cost indoor positioning systems"</i>, In Proceedings of the IEEE International Conference on Robotics and Automation. , pp. 6269-6275.
  <p class="infolinks">[<a href="javascript:toggleInfo('GhaffariJadidi2017a','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('GhaffariJadidi2017a','bibtex')">BibTeX</a>] [<a href="https://arxiv.org/pdf/1609.03130.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_GhaffariJadidi2017a" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, we propose a real-time classification scheme to cope with noisy Radio Signal Strength Indicator (RSSI) measurements utilized in indoor positioning systems. RSSI values are often converted to distances for position estimation. However due to multipathing and shadowing effects, finding a unique sensor model using both parametric and non-parametric methods is highly challenging. We learn decision regions using the Gaussian Processes classification to accept measurements that are consistent with the operating sensor model. The proposed approach can perform online, does not rely on a particular sensor model or parameters, and is robust to sensor failures. The experimental results achieved using hardware show that available positioning algorithms can benefit from incorporating the classifier into their measurement model as a meta-sensor modeling technique.</td>
</tr>
<tr id="bib_GhaffariJadidi2017a" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{GhaffariJadidi2017a,
  author = {Ghaffari Jadidi, Maani and Patel, Mitesh and Valls Miro, Jaime},
  title = {Gaussian processes online observation classification for RSSI-based low-cost indoor positioning systems},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation},
  year = {2017},
  pages = {6269--6275},
  url = {https://arxiv.org/pdf/1609.03130.pdf}
}
</pre></td>
</tr>
<tr id="Emery2016" data-reftype="InProceedings" class="entry">
	<td>Emery BM, Ghaffari Jadidi M, Nakamura K and Valls Miro J (2016), <i>"An audio-visual solution to sound source localization and tracking with applications to HRI"</i>, In Proceedings of the Australasian Conference on Robotics and Automation. , pp. 1-10.
  <p class="infolinks">[<a href="javascript:toggleInfo('Emery2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Emery2016','bibtex')">BibTeX</a>] [<a href="https://opus.lib.uts.edu.au/bitstream/10453/130936/1/ACRA16.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_Emery2016" class="abstract noshow">
	<td><b>Abstract</b>: Robot audition is an emerging and growing branch in the robotic community and is necessary for a natural Human-Robot Interaction (HRI). In this paper, we propose a framework that integrates advances from Simultaneous Localization And Mapping (SLAM), bearing-only target tracking, and robot audition techniques into a unifed system for sound source identification, localization, and tracking. In indoors, acoustic observations are often highly noisy and corrupted due to reverberations, the robot ego-motion and background noise, and possible discontinuous nature of them. Therefore, in everyday interaction scenarios, the system requires accommodating for outliers, robust data association, and appropriate management of the landmarks, i.e. sound sources. We solve the robot self-localization and environment representation problems using an RGB-D SLAM algorithm, and sound source localization and tracking using recursive Bayesian estimation in the form of the extended Kalman Filter with unknown data associations and an unknown number of landmarks. The experimental results show that the proposed system performs well in the medium-sized cluttered indoor environment.</td>
</tr>
<tr id="bib_Emery2016" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Emery2016,
  author = {Emery, Brendan M. and Ghaffari Jadidi, Maani and Nakamura, Keisuke and Valls Miro, Jaime},
  title = {An audio-visual solution to sound source localization and tracking with applications to HRI},
  booktitle = {Proceedings of the Australasian Conference on Robotics and Automation},
  year = {2016},
  pages = {1--10},
  url = {https://opus.lib.uts.edu.au/bitstream/10453/130936/1/ACRA16.pdf}
}
</pre></td>
</tr>
<tr id="GhaffariJadidi2015" data-reftype="InProceedings" class="entry">
	<td>Ghaffari Jadidi M, Valls Miro J and Dissanayake G (2015), <i>"Mutual information-based exploration on continuous occupancy maps"</i>, In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. , pp. 6086-6092.
  <p class="infolinks">[<a href="javascript:toggleInfo('GhaffariJadidi2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('GhaffariJadidi2015','bibtex')">BibTeX</a>] [<a href="https://opus.lib.uts.edu.au/bitstream/10453/39971/6/Mutual%20Information-based%20Exploration.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_GhaffariJadidi2015" class="abstract noshow">
	<td><b>Abstract</b>: The problem of active perception with an autonomous robot is studied in this paper. It is proposed that the exploratory behavior of the robot be controlled using mutual information (MI) surfaces between the current map and a one-step look ahead measurements. MI surfaces highlight informative areas for exploration. A novel method for computing these surfaces is described. An approach that exploits structural dependencies of the environment and handles sparse sensor measurements to build a continuous model of the environment, that can then be used to generate MI surfaces is also proposed. A gradient field of occupancy probability distribution is regressed from sensor data as a Gaussian Process and provide frontier boundaries for further exploration. The continuous global frontier surface completely describes unexplored regions and, inherently, provides an automatic termination criterion for a desired sensitivity. The results from publicly available datasets confirm an average improvement of the proposed methodology over comparable standard and state-of-the-art exploratory methods available in the literature by more than 20% and 13% in travel distance and map entropy reduction rate, respectively.</td>
</tr>
<tr id="bib_GhaffariJadidi2015" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{GhaffariJadidi2015,
  author = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Dissanayake, Gamini},
  title = {Mutual information-based exploration on continuous occupancy maps},
  booktitle = {Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2015},
  pages = {6086--6092},
  url = {https://opus.lib.uts.edu.au/bitstream/10453/39971/6/Mutual%20Information-based%20Exploration.pdf}
}
</pre></td>
</tr>
<tr id="GhaffariJadidi2014" data-reftype="InProceedings" class="entry">
	<td>Ghaffari Jadidi M, Valls Miro J, Valencia R and Andrade-Cetto J (2014), <i>"Exploration on continuous Gaussian process frontier maps"</i>, In Proceedings of the IEEE International Conference on Robotics and Automation. , pp. 6077-6082.
  <p class="infolinks">[<a href="javascript:toggleInfo('GhaffariJadidi2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('GhaffariJadidi2014','bibtex')">BibTeX</a>] [<a href="https://upcommons.upc.edu/bitstream/handle/2117/28286/1497-Exploration-on-continuous-Gaussian-process-frontier-maps.pdf%3Bjsessionid%3DE2AD0F2DA15B663C329548679C443986?sequence%3D1" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_GhaffariJadidi2014" class="abstract noshow">
	<td><b>Abstract</b>: An information-driven autonomous robotic exploration method on a continuous representation of unknown environments is proposed in this paper. The approach conveniently handles sparse sensor measurements to build a continuous model of the environment that exploits structural dependencies without the need to resort to a fixed resolution grid map. A gradient field of occupancy probability distribution is regressed from sensor data as a Gaussian process providing frontier boundaries for further exploration. The resulting continuous global frontier surface completely describes unexplored regions and, inherently, provides an automatic stop criterion for a desired sensitivity. The performance of the proposed approach is evaluated through simulation results in the well-known Freiburg and Cave maps.</td>
</tr>
<tr id="bib_GhaffariJadidi2014" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{GhaffariJadidi2014,
  author = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Valencia, Rafael and Andrade-Cetto, Juan},
  title = {Exploration on continuous Gaussian process frontier maps},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation},
  year = {2014},
  pages = {6077--6082},
  url = {https://upcommons.upc.edu/bitstream/handle/2117/28286/1497-Exploration-on-continuous-Gaussian-process-frontier-maps.pdf%3Bjsessionid%3DE2AD0F2DA15B663C329548679C443986?sequence%3D1}
}
</pre></td>
</tr>
<tr id="GhaffariJadidi2013" data-reftype="InProceedings" class="entry">
	<td>Ghaffari Jadidi M, Valls Miro J, Valencia Carreno R, Andrade-Cetto J and Dissanayake G (2013), <i>"Exploration in Information Distribution Maps"</i>, In RSS Workshop on Robotic Exploration, Monitoring, and Information Collection. Berlin, Germany , pp. 1-8.
  <p class="infolinks">[<a href="javascript:toggleInfo('GhaffariJadidi2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('GhaffariJadidi2013','bibtex')">BibTeX</a>] [<a href="http://www.iri.upc.edu/files/scidoc/1428-Exploration-in-Information-Distribution-Maps.pdf" target="_blank">URL</a>]</p>
	</td>
</tr>
<tr id="abs_GhaffariJadidi2013" class="abstract noshow">
	<td><b>Abstract</b>: In this paper, a novel solution for autonomous robotic exploration is proposed. We model the distribution of information in an unknown environment as an unsteady diffusion process, which can be an appropriate mathematical formulation and analogy for expanding, time-varying, and dynamic envi-ronments. This information distribution map is the solution of the diffusion process partial differential equation, and is regressed from sensor data as a Gaussian Process. Optimization of the process parameters leads to an optimal frontier map which describes regions of interest for further exploration. Since the presented approach considers a continuous model of the environment, it can be used to plan smooth exploration paths exploiting the structural dependencies of the environment whilst handling sparse sensor measurements. The performance of the approach is evaluated through simulation results in the well-known Freiburg and Cave maps.</td>
</tr>
<tr id="bib_GhaffariJadidi2013" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{GhaffariJadidi2013,
  author = {Ghaffari Jadidi, Maani and Valls Miro, Jaime and Valencia Carreno, Rafael and Andrade-Cetto, Juan and Dissanayake, Gamini},
  title = {Exploration in Information Distribution Maps},
  booktitle = {RSS Workshop on Robotic Exploration, Monitoring, and Information Collection},
  year = {2013},
  pages = {1--8},
  url = {http://www.iri.upc.edu/files/scidoc/1428-Exploration-in-Information-Distribution-Maps.pdf}
}
</pre></td>
</tr>
<tr id="Hashemi2009" data-reftype="InProceedings" class="entry">
	<td>Hashemi E, Ghaffari Jadidi M and Babarsad OB (2009), <i>"Trajectory planning optimization with dynamic modeling of four wheeled omni-directional mobile robots"</i>, In IEEE International Symposium on Computational Intelligence in Robotics and Automation. , pp. 272-277.
  <p class="infolinks">[<a href="javascript:toggleInfo('Hashemi2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hashemi2009','bibtex')">BibTeX</a>] [<a href="https://doi.org/10.1109/CIRA.2009.5423195" target="_blank">DOI</a>]</p>
	</td>
</tr>
<tr id="abs_Hashemi2009" class="abstract noshow">
	<td><b>Abstract</b>: Path planning together with the tuning and determination of controller parameters are major concerns in omnidirectional mobile robots. Defining appropriate controller parameters in acceleration and deceleration to reach far and near target points without slippage is one of critical issues since some troubles due to unregulated velocities may greatly affect the ability of robot for the specified path planning and attaining the mentioned targets. A robot accurate kinematic and dynamic modeling and simulation accompanied by velocity and acceleration filtering are mainly discussed in this paper. Major changes and improvements in motion analysis, simulation and accuracy for the newly presented model and its efficiency are discussed in comparison with the previous simple kinematic modeling. Employing the new approach for robot dynamic modeling, particularly acceleration filtering, results in to the more precise robot control and achieving appropriate results.</td>
</tr>
<tr id="bib_Hashemi2009" class="bibtex noshow">
<td><b>BibTeX</b>:
<pre>
@inproceedings{Hashemi2009,
  author = {Hashemi, Ehsan and Ghaffari Jadidi, Maani and Babarsad, Omid Bakhshandeh},
  title = {Trajectory planning optimization with dynamic modeling of four wheeled omni-directional mobile robots},
  booktitle = {IEEE International Symposium on Computational Intelligence in Robotics and Automation},
  year = {2009},
  pages = {272--277},
  doi = {10.1109/CIRA.2009.5423195}
}
</pre></td>
</tr>
</tbody>
</table>
</body>
</html>
